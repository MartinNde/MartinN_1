{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinNde/MartinN_1/blob/main/Challenge_4_AustenWordLevelTextGen_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFCim8iZgH"
      },
      "source": [
        "#Text Generation in the style of Jane Austen -  Exercise Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_0F75kzfNuV"
      },
      "source": [
        "## 1. Change the hyper parameters, including the size and number of LSTM layers and number of epochs to see if you get better results.\n",
        "\n",
        "Smplify the network to one LSTM layer and increase the epochs to 50 improves Bleu score to 0.95. More epochs will improve this further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUfDqRKgiUZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7d9e09-2305-400f-b8ea-aaea71b97984"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('blake-poems.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=50, verbose=1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 389)               311589    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,877,989\n",
            "Trainable params: 2,877,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "15/15 [==============================] - 9s 47ms/step - loss: 5.8248\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 5.4685\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3571\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 5.3477\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3398\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3373\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 5.3256\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3211\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 5.3186\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 5.3177\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 1s 41ms/step - loss: 5.3191\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3174\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3104\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3130\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3103\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3098\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3051\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3069\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3052\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3036\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3064\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2986\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3010\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2978\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2967\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2964\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2922\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2931\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2905\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2879\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2859\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2773\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2754\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2721\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.2554\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2509\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.2352\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2275\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.1993\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.1625\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.1307\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.0517\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.0096\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 4.8993\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 4.7907\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 4.6621\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 4.5135\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 4.3620\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 4.1829\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 3.9906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30b80ff850>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKUH5aycmUr",
        "outputId": "7d2816b4-e696-4e3f-ee4e-3e40e3e9f957"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "little lamb who make thee dost thou know who made thee gave thee life and bid thee feed by the stream and er the mead gave thee clothing of delight softest clothing wolly bright gave thee such tender voice making all the vales rejoice little lamb who made thee dost thou know who made thee little lamb ll tell thee little lamb ll tell thee he is called by thy name for he calls himself lamb he is meek and he is mild he became little child a child and thou lamb we are called by his name little lamb\n",
            "Seed word sequence: little lamb who make thee dost thou know who made thee gave thee life and bid thee feed by the stream and er the mead gave thee clothing of delight softest clothing wolly bright gave thee such tender voice making all the vales rejoice little lamb who made thee dost thou know who made thee little lamb ll tell thee little lamb ll tell thee he is called by thy name for he calls himself lamb he is meek and he is mild he became little child a child and thou lamb we are called by his name little lamb\n",
            "Predicted words: lamb thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee\n",
            "BLEU Score for predicted words: 0.2214904814361361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0SDqaoFhB8C"
      },
      "source": [
        "## 2. Try adding dropout after the LSTM layers and Dense layers.\n",
        "\n",
        "In this particular model, Dropout prevents the model from learning at 50 epochs, but it does learn by 85 epochs. Dropout slows learning down in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdc-iC_hK1C",
        "outputId": "d3b4171d-a536-4c18-b2fb-1c6ce5a005b5"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 800)               2566400   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 800)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 3s 53ms/step - loss: 5.5880\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.2549\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.1425\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.1223\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.1036\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.1092\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0874\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0992\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0833\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0740\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0755\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0717\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0968\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0619\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0796\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0747\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0714\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0810\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0636\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0610\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0724\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0623\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0554\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0580\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0535\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0575\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0602\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0591\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0503\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0481\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0623\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0446\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0526\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0455\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0326\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0558\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0331\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0382\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0360\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0434\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0260\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0501\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0365\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0284\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0158\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0185\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0190\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0089\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9993\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9910\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0081\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9810\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.9628\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.9557\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9353\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9215\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9052\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9075\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.8673\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.8250\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.8053\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.7706\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.7168\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.6504\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.5794\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.5026\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.4266\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.3292\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.2062\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.1097\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.9707\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 3.8177\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.6584\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.4886\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.2036\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.2089\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 2.9581\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.6467\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 2.3996\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.1687\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.9143\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.6696\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.3568\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.1028\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.8538\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.6521\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.5133\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.4411\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 3.1565\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 7.1885\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 6.0928\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 6.0512\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 6.0035\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.7429\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.6411\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.7787\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.6743\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.5760\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.6641\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.5350\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30ac08a760>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybvYRKc7hNgF",
        "outputId": "df15a8a8-a321-481e-ae4b-8fce46861cfe"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relish to his existence by former marriage mr henry dashwood had one son by his present lady three daughters the son steady respectable young man was amply provided for by the fortune of his mother which had been large and half of which devolved on him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the norland estate was not so really important as to his sisters for their fortune independent of what might arise to them from their father inheriting that property could\n",
            "Seed word sequence: relish to his existence by former marriage mr henry dashwood had one son by his present lady three daughters the son steady respectable young man was amply provided for by the fortune of his mother which had been large and half of which devolved on him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the norland estate was not so really important as to his sisters for their fortune independent of what might arise to them from their father inheriting that property could\n",
            "Predicted words: his his his his his his his his his to his his his his his his his his his his his his his his to his his his to to his his his his his his to his his his his his his his his his to his his his his his to his his his his his his his his his his his his his his his his his his to his to his to his his his to his his his to to his his his his his his to his his his his his his his his\n",
            "BLEU Score for predicted words: 0.1849574339867536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdtLURYijpa"
      },
      "source": [
        "## 3. Normalisation does not always provide the best results. Remove normalisation and see if this improves the results (this will probably mean the model hyper-parameters also need changing).\n",
        "\n",
        "The model has learnt by 20 epochs, so not having normalisation has sped up the model training.  The number of neurons in the model can also be reduced but this increases the number of epochs it takes to train.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FGGjud3imPQ",
        "outputId": "67c5ad39-2cf0-41f6-b9ac-e3818cc53fe1"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "# X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "13/13 [==============================] - 3s 50ms/step - loss: 5.5200\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.9410\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.7604\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.6200\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.4564\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.2602\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.0423\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.7916\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.4895\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.1354\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.7706\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.3393\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.9800\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.6078\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.2845\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.0071\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.7981\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.6239\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.4756\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.3730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30a7e9be80>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccz-a4g8imvh",
        "outputId": "6fb0347f-ae3b-4ea7-913c-d2d23ab1d4e0"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "promised to do every thing in his power to make them comfortable his father was rendered easy by such an assurance and mr john dashwood had then leisure to consider how much there might prudently be in his power to do for them he was not an ill disposed young man unless to be rather cold hearted and rather selfish is to be ill disposed but he was in general well respected for he conducted himself with propriety in the discharge of his ordinary duties had he married more amiable woman he might have been made still more respectable than\n",
            "Seed word sequence: promised to do every thing in his power to make them comfortable his father was rendered easy by such an assurance and mr john dashwood had then leisure to consider how much there might prudently be in his power to do for them he was not an ill disposed young man unless to be rather cold hearted and rather selfish is to be ill disposed but he was in general well respected for he conducted himself with propriety in the discharge of his ordinary duties had he married more amiable woman he might have been made still more respectable than\n",
            "Predicted words: he was he might even have been made amiable himself for he was very young when he married and very fond of his wife but mrs john dashwood was strong caricature of himself more narrow minded and selfish when he gave his promise to his father he meditated within himself to increase of fortunes of his sisters by the present of thousand pounds piece he then really thought himself equal to it the prospect of four thousand year in addition to his pr of his ordinary of his three girls he left them thousand pounds piece mr dashwood disappointment was\n",
            "BLEU Score for predicted words: 0.9802228716555276\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCwKDdQEkLv0"
      },
      "source": [
        "## 4. Add an Embedding Layer into the DNN to see if this improves the model.\n",
        "\n",
        "Adding an embedding layer and removing normalisation allows the model to acheive good results after 35 epochs.\n",
        "\n",
        "The dimension of the dense embedding layer was arbitarily selected to be 50.  This is a parameter that can be experimented with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znTg1sDZkNPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d818ca3-8dbb-4a5d-a75a-2d33574deb6d"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "#X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 50)           17200     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 800)               2723200   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,015,944\n",
            "Trainable params: 3,015,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 4s 160ms/step - loss: 5.6265\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 1s 86ms/step - loss: 5.2365\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 1s 89ms/step - loss: 5.1219\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 1s 59ms/step - loss: 5.0796\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 5.0728\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 1s 80ms/step - loss: 5.0618\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0544\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0320\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 1s 79ms/step - loss: 5.0143\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.9780\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.9380\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0756\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.8255\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.6803\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 58ms/step - loss: 4.4946\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.2473\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.0169\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 1s 68ms/step - loss: 3.6847\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 3.3154\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 2.9393\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 2.4943\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 2.0013\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 1.5611\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 1.6046\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 1.3155\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.8292\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.5183\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 1s 59ms/step - loss: 0.3361\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.2267\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.1544\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.1178\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0912\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0756\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0643\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0548\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0475\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0421\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0380\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0347\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0317\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0293\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0271\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0251\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0234\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0219\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0206\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0194\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0183\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0173\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0163\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0155\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0148\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0141\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0134\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0128\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0122\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0117\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0112\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 1s 64ms/step - loss: 0.0108\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0104\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0100\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0096\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0092\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0089\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0086\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0083\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0080\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0078\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0075\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0073\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0070\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0068\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0066\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0064\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0062\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0061\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0059\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0057\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0056\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0054\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 1s 56ms/step - loss: 0.0053\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0051\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0050\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0049\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0047\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 0.0046\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0045\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0044\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0043\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0042\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0041\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0040\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0039\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0038\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0037\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0037\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0036\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0035\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0034\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0034\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30a7aa7670>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE0y1mDYkNiG",
        "outputId": "654478da-497d-4052-b582-ae4ae25bfc67"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the norland estate was not so really important as to his sisters for their fortune independent of what might arise to them from their father inheriting that property could be but small their mother had nothing and their father only seven thousand pounds in his own disposal for the remaining moiety of his first wife fortune was also secured to her child and he had only life interest in it the old gentleman died\n",
            "Seed word sequence: him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the norland estate was not so really important as to his sisters for their fortune independent of what might arise to them from their father inheriting that property could be but small their mother had nothing and their father only seven thousand pounds in his own disposal for the remaining moiety of his first wife fortune was also secured to her child and he had only life interest in it the old gentleman died\n",
            "Predicted words: his will was read and like almost every other will gave as much disappointment as pleasure he was neither so unjust nor so ungrateful as to leave his estate from his nephew but he left it to him on such terms as destroyed half the value of the bequest mr dashwood had wished for it more for the sake of his wife and daughters than for himself or his son but to his son and his son son child of four years old it was secured in such way as to leave to himself no power of providing for those\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ]
    }
  ]
}