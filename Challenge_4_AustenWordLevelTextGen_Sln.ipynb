{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinNde/MartinN_1/blob/main/Challenge_4_AustenWordLevelTextGen_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFCim8iZgH"
      },
      "source": [
        "#Text Generation in the style of Jane Austen -  Exercise Solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_0F75kzfNuV"
      },
      "source": [
        "## 1. Change the hyper parameters, including the size and number of LSTM layers and number of epochs to see if you get better results.\n",
        "\n",
        "Smplify the network to one LSTM layer and increase the epochs to 50 improves Bleu score to 0.95. More epochs will improve this further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUfDqRKgiUZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d88d72-0de7-45e6-8108-145a6202a27f"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('blake-poems.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=50, verbose=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_13 (LSTM)              (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 389)               311589    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,877,989\n",
            "Trainable params: 2,877,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "15/15 [==============================] - 3s 47ms/step - loss: 5.8758\n",
            "Epoch 2/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.4510\n",
            "Epoch 3/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3615\n",
            "Epoch 4/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3452\n",
            "Epoch 5/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3411\n",
            "Epoch 6/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3337\n",
            "Epoch 7/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3264\n",
            "Epoch 8/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3231\n",
            "Epoch 9/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3218\n",
            "Epoch 10/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3168\n",
            "Epoch 11/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3172\n",
            "Epoch 12/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3171\n",
            "Epoch 13/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3163\n",
            "Epoch 14/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3127\n",
            "Epoch 15/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3097\n",
            "Epoch 16/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3088\n",
            "Epoch 17/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3097\n",
            "Epoch 18/50\n",
            "15/15 [==============================] - 1s 42ms/step - loss: 5.3096\n",
            "Epoch 19/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3019\n",
            "Epoch 20/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3053\n",
            "Epoch 21/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.3024\n",
            "Epoch 22/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3011\n",
            "Epoch 23/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.3014\n",
            "Epoch 24/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2971\n",
            "Epoch 25/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2958\n",
            "Epoch 26/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2981\n",
            "Epoch 27/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2870\n",
            "Epoch 28/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2908\n",
            "Epoch 29/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2864\n",
            "Epoch 30/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2787\n",
            "Epoch 31/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.2758\n",
            "Epoch 32/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2829\n",
            "Epoch 33/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2693\n",
            "Epoch 34/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2623\n",
            "Epoch 35/50\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 5.2548\n",
            "Epoch 36/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.2496\n",
            "Epoch 37/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.2427\n",
            "Epoch 38/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.2285\n",
            "Epoch 39/50\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 5.2002\n",
            "Epoch 40/50\n",
            "15/15 [==============================] - 1s 45ms/step - loss: 5.1791\n",
            "Epoch 41/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.1338\n",
            "Epoch 42/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.0671\n",
            "Epoch 43/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.0201\n",
            "Epoch 44/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 5.0050\n",
            "Epoch 45/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.9076\n",
            "Epoch 46/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.7966\n",
            "Epoch 47/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.7415\n",
            "Epoch 48/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.5910\n",
            "Epoch 49/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.4687\n",
            "Epoch 50/50\n",
            "15/15 [==============================] - 1s 44ms/step - loss: 4.2988\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fc0790370>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKUH5aycmUr",
        "outputId": "e05ef583-7d56-47e1-c8e3-f3effc61ceec"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "an end round the laps of their mothers many sisters and brothers like birds in their nest are ready for rest and sport no more seen on the darkening green the lamb little lamb who make thee dost thou know who made thee gave thee life and bid thee feed by the stream and er the mead gave thee clothing of delight softest clothing wolly bright gave thee such tender voice making all the vales rejoice little lamb who made thee dost thou know who made thee little lamb ll tell thee little lamb ll tell thee he is called\n",
            "Seed word sequence: an end round the laps of their mothers many sisters and brothers like birds in their nest are ready for rest and sport no more seen on the darkening green the lamb little lamb who make thee dost thou know who made thee gave thee life and bid thee feed by the stream and er the mead gave thee clothing of delight softest clothing wolly bright gave thee such tender voice making all the vales rejoice little lamb who made thee dost thou know who made thee little lamb ll tell thee little lamb ll tell thee he is called\n",
            "Predicted words: thee he lamb the thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee thee\n",
            "BLEU Score for predicted words: 0.23652441512110797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0SDqaoFhB8C"
      },
      "source": [
        "## 2. Try adding dropout after the LSTM layers and Dense layers.\n",
        "\n",
        "In this particular model, Dropout prevents the model from learning at 50 epochs, but it does learn by 85 epochs. Dropout slows learning down in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdc-iC_hK1C",
        "outputId": "243c3e6d-6ee3-4bf5-e6fa-5c9778f846dc"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_17 (LSTM)              (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 2s 52ms/step - loss: 5.6276\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.2197\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.1134\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.0898\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.0758\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.0770\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.0741\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0713\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 5.0710\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0655\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 42ms/step - loss: 5.0612\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0612\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0582\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 5.0615\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 51ms/step - loss: 5.0584\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 5.0551\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0535\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0513\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0560\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 5.0495\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0490\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 5.0457\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0469\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0496\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 5.0449\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 1s 51ms/step - loss: 5.0470\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 5.0354\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 5.0411\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0390\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0308\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0290\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0200\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0168\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0103\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0087\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9879\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.9904\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.9701\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9546\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.9339\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.9117\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.8817\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.8357\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.7961\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.7379\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.6681\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.5952\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.5028\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.3833\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.2944\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.1213\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.0274\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 3.8504\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 3.6145\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 3.3454\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 3.1020\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 2.7698\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 2.4087\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 2.1094\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.7134\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.3388\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.0177\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.7600\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.5159\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.3318\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.2140\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.1454\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0989\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0767\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0581\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0473\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0400\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0351\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0310\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0280\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0253\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0232\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0214\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0198\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0184\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0172\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0160\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0151\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0142\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0134\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0126\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0120\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0114\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0109\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0103\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0099\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0094\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0090\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0087\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0083\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0080\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0077\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0074\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0071\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.0069\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fb0584280>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybvYRKc7hNgF",
        "outputId": "25f91996-8b22-477c-9015-c221db5bcf67"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "was large and their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his\n",
            "Seed word sequence: was large and their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his\n",
            "Predicted words: nephew mr henry dashwood the legal inheritor of the norland estate and the person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children added relish to his existence by former marriage mr henry dashwood had one son\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdtLURYijpa"
      },
      "source": [
        "## 3. Normalisation does not always provide the best results. Remove normalisation and see if this improves the results (this will probably mean the model hyper-parameters also need changing).\n",
        "\n",
        "The model has learnt by 20 epochs, so not having normalisation has sped up the model training.  The number of neurons in the model can also be reduced but this increases the number of epochs it takes to train.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FGGjud3imPQ",
        "outputId": "3d31b2c7-eaf0-4494-c216-86aee089d72e"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "# X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_11 (LSTM)              (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "13/13 [==============================] - 2s 50ms/step - loss: 5.4797\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.9085\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.7512\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.5853\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.4221\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.2303\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 1s 43ms/step - loss: 4.0073\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.7517\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.4485\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.1446\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.7907\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.4230\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 2.0572\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.7229\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 1.4117\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.1131\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.8701\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.6800\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.5336\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 0.4147\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9ff16ab730>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccz-a4g8imvh",
        "outputId": "6309e60a-3972-4415-f9a1-7f8a131d5e0b"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "only life interest in it the old gentleman died his will was read and like almost every other will gave as much disappointment as pleasure he was neither so unjust nor so ungrateful as to leave his estate from his nephew but he left it to him on such terms as destroyed half the value of the bequest mr dashwood had wished for it more for the sake of his wife and daughters than for himself or his son but to his son and his son son child of four years old it was secured in such way as to\n",
            "Seed word sequence: only life interest in it the old gentleman died his will was read and like almost every other will gave as much disappointment as pleasure he was neither so unjust nor so ungrateful as to leave his estate from his nephew but he left it to him on such terms as destroyed half the value of the bequest mr dashwood had wished for it more for the sake of his wife and daughters than for himself or his son but to his son and his son son child of four years old it was secured in such way as to\n",
            "Predicted words: leave to himself no power of providing for those who were most dear to him and who most needed provision by any charge of any charge of the benefit of the ordinary duties by the married more amiable woman he might have been made still more respectable than he was he might even have been made amiable himself for he was very young when he married and very fond of his wife but mrs john dashwood was strong caricature of himself more narrow minded and selfish when he gave his promise to his father he meditated within himself to increase\n",
            "BLEU Score for predicted words: 0.9878180719132127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCwKDdQEkLv0"
      },
      "source": [
        "## 4. Add an Embedding Layer into the DNN to see if this improves the model.\n",
        "\n",
        "Adding an embedding layer and removing normalisation allows the model to acheive good results after 35 epochs.\n",
        "\n",
        "The dimension of the dense embedding layer was arbitarily selected to be 50.  This is a parameter that can be experimented with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znTg1sDZkNPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48026746-cbd6-4c9d-ab53-5bebcbe34304"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "#X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 50)           17200     \n",
            "                                                                 \n",
            " lstm_12 (LSTM)              (None, 800)               2723200   \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,015,944\n",
            "Trainable params: 3,015,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 4s 175ms/step - loss: 5.6200\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 1s 101ms/step - loss: 5.2533\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 1s 56ms/step - loss: 5.1323\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 1s 56ms/step - loss: 5.0869\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 1s 68ms/step - loss: 5.0739\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 5.0596\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0475\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 5.0371\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0241\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 1s 67ms/step - loss: 4.9952\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 5.0629\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 4.8927\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.7933\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.6538\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 60ms/step - loss: 4.4723\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.1855\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 3.8753\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 3.5437\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 3.1383\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 2.7143\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 2.2550\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 1.7421\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 1.2575\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.8584\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 1s 58ms/step - loss: 0.5410\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 1s 58ms/step - loss: 0.3214\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.1925\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.1230\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0850\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 1s 58ms/step - loss: 0.0649\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 0.0530\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 1s 57ms/step - loss: 0.0450\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0386\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0346\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0310\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0279\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0255\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0235\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0215\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0200\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0187\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0175\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0164\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0154\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0145\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0137\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0130\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0123\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0117\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0111\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0106\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0101\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0097\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0093\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0089\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0085\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0082\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0079\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0076\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0073\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0070\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0068\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0065\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0063\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0061\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0059\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 1s 68ms/step - loss: 0.0057\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0055\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0054\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0052\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0051\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0049\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0048\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0046\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0045\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0044\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0043\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0041\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0040\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0039\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0038\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0037\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0036\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0036\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0035\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0034\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0033\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0032\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0032\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0031\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0030\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0030\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0029\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0028\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0028\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0027\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0026\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0026\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0025\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0025\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9fee145f40>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE0y1mDYkNiG",
        "outputId": "f47fd40d-6af9-4e6e-bf8c-85dc85d27977"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his nephew mr henry\n",
            "Seed word sequence: their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his nephew mr henry\n",
            "Predicted words: dashwood the legal inheritor of the norland estate and the person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children added relish to his existence by former marriage mr henry dashwood had one son by his present\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ]
    }
  ]
}