{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinNde/MartinN_1/blob/main/Challenge_4_AustenWordLevelTextGen_Sln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKqFCim8iZgH"
      },
      "source": [
        "#Poem Generation in the style of William Blake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_0F75kzfNuV"
      },
      "source": [
        "## 1. Change the hyper parameters, including the size and number of LSTM layers and number of epochs to see if you get better results.\n",
        "\n",
        "One LSTM layer and increase the epochs to 100 improves gives a Bleu score to 1.00"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUfDqRKgiUZ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9fb7b9-962d-45b7-bbb7-fb454f05eab0"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('blake-poems.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "opt = RMSprop(lr=0.001)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1],activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=68, epochs=100, verbose=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_20 (LSTM)              (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 389)               311589    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,877,989\n",
            "Trainable params: 2,877,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "14/14 [==============================] - 3s 60ms/step - loss: 5.8437\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 1s 55ms/step - loss: 5.4828\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 5.3806\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.3340\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.3388\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 1s 55ms/step - loss: 5.3305\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 5.3194\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 5.3180\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.3180\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.3166\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 1s 55ms/step - loss: 5.3127\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 5.3084\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 5.3093\n",
            "Epoch 14/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 5.3103\n",
            "Epoch 15/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 5.3060\n",
            "Epoch 16/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.3057\n",
            "Epoch 17/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.3049\n",
            "Epoch 18/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.3050\n",
            "Epoch 19/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 5.3070\n",
            "Epoch 20/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.3029\n",
            "Epoch 21/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 5.2981\n",
            "Epoch 22/100\n",
            "14/14 [==============================] - 1s 55ms/step - loss: 5.2948\n",
            "Epoch 23/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2953\n",
            "Epoch 24/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.2947\n",
            "Epoch 25/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.2939\n",
            "Epoch 26/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.2937\n",
            "Epoch 27/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2921\n",
            "Epoch 28/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2920\n",
            "Epoch 29/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2833\n",
            "Epoch 30/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2856\n",
            "Epoch 31/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2767\n",
            "Epoch 32/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2748\n",
            "Epoch 33/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2667\n",
            "Epoch 34/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2569\n",
            "Epoch 35/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2581\n",
            "Epoch 36/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2355\n",
            "Epoch 37/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2287\n",
            "Epoch 38/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.2042\n",
            "Epoch 39/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.1705\n",
            "Epoch 40/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.1033\n",
            "Epoch 41/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 5.0701\n",
            "Epoch 42/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 5.0354\n",
            "Epoch 43/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.9631\n",
            "Epoch 44/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.8683\n",
            "Epoch 45/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.7829\n",
            "Epoch 46/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.6339\n",
            "Epoch 47/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.6895\n",
            "Epoch 48/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.5710\n",
            "Epoch 49/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.3586\n",
            "Epoch 50/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 4.1782\n",
            "Epoch 51/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 4.1035\n",
            "Epoch 52/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 3.9985\n",
            "Epoch 53/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 3.7611\n",
            "Epoch 54/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 3.4294\n",
            "Epoch 55/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 3.1167\n",
            "Epoch 56/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 2.8194\n",
            "Epoch 57/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 2.5265\n",
            "Epoch 58/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 2.2055\n",
            "Epoch 59/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 1.9116\n",
            "Epoch 60/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 1.5084\n",
            "Epoch 61/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 1.1624\n",
            "Epoch 62/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.9782\n",
            "Epoch 63/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.6913\n",
            "Epoch 64/100\n",
            "14/14 [==============================] - 1s 57ms/step - loss: 0.4614\n",
            "Epoch 65/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.2886\n",
            "Epoch 66/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.1830\n",
            "Epoch 67/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.1210\n",
            "Epoch 68/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0888\n",
            "Epoch 69/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0652\n",
            "Epoch 70/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0487\n",
            "Epoch 71/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0394\n",
            "Epoch 72/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0332\n",
            "Epoch 73/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0288\n",
            "Epoch 74/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0255\n",
            "Epoch 75/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0230\n",
            "Epoch 76/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0208\n",
            "Epoch 77/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0190\n",
            "Epoch 78/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0175\n",
            "Epoch 79/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 0.0162\n",
            "Epoch 80/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 0.0150\n",
            "Epoch 81/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0140\n",
            "Epoch 82/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0131\n",
            "Epoch 83/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0123\n",
            "Epoch 84/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0115\n",
            "Epoch 85/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0109\n",
            "Epoch 86/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0103\n",
            "Epoch 87/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0098\n",
            "Epoch 88/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 0.0093\n",
            "Epoch 89/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0088\n",
            "Epoch 90/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0084\n",
            "Epoch 91/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0080\n",
            "Epoch 92/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0077\n",
            "Epoch 93/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 0.0073\n",
            "Epoch 94/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0070\n",
            "Epoch 95/100\n",
            "14/14 [==============================] - 1s 59ms/step - loss: 0.0067\n",
            "Epoch 96/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0065\n",
            "Epoch 97/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0062\n",
            "Epoch 98/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0060\n",
            "Epoch 99/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0058\n",
            "Epoch 100/100\n",
            "14/14 [==============================] - 1s 58ms/step - loss: 0.0056\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0efa36ffa0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgKUH5aycmUr",
        "outputId": "914cfa15-dc94-4c79-aac8-a6cac22afb77"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bush sing louder around to the bells cheerful sound while our sports shall be seen on the echoing green old john with white hair does laugh away care sitting under the oak among the old folk they laugh at our play and soon they all say such such were the joys when we all girls and boys in our youth time were seen on the echoing green till the little ones weary no more can be merry the sun does descend and our sports have an end round the laps of their mothers many sisters and brothers like birds\n",
            "Seed word sequence: the bush sing louder around to the bells cheerful sound while our sports shall be seen on the echoing green old john with white hair does laugh away care sitting under the oak among the old folk they laugh at our play and soon they all say such such were the joys when we all girls and boys in our youth time were seen on the echoing green till the little ones weary no more can be merry the sun does descend and our sports have an end round the laps of their mothers many sisters and brothers like birds\n",
            "Predicted words: in their nest are ready for rest and sport no more seen on the darkening green the lamb little lamb who make thee dost thou know who made thee gave thee life and bid thee feed by the stream and er the mead gave thee clothing of delight softest clothing wolly bright gave thee such tender voice making all the vales rejoice little lamb who made thee dost thou know who made thee little lamb ll tell thee little lamb ll tell thee he is called by thy name for he calls himself lamb he is meek and he is\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0SDqaoFhB8C"
      },
      "source": [
        "\n",
        "Dropout is removed since it reduces the efficiency of the model, the epocs are increase since more epocs give better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsdc-iC_hK1C",
        "outputId": "dd5931bc-b431-4d5b-c40d-049c78818432"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_21\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_21 (LSTM)              (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 3s 50ms/step - loss: 5.5996\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.1969\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.1149\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0875\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0734\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 5.0708\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0639\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0690\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0619\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0639\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0603\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0593\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0605\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0536\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0536\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0525\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0484\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0494\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0489\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0442\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 5.0444\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0443\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0379\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0352\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0349\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0328\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0312\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0254\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0207\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 5.0118\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0104\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 5.0016\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 4.9837\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.9705\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 4.9481\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.9187\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.8841\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 4.8290\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.7590\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.6865\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.6031\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.4923\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.4179\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.3733\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.3264\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.1393\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 3.9389\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 3.7659\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 3.5452\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 3.3274\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 3.0386\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 2.7125\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 2.4416\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 2.0973\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 1.7097\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 1.3647\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 1.0132\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.7329\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.4809\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.3239\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.2006\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.1443\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.1043\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.7687\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.1046\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0701\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0471\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0386\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0330\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0265\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0219\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0194\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0177\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0162\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0149\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0138\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0129\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0121\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0113\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0107\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0101\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0096\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0091\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0087\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0083\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0079\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0076\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0073\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0070\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0067\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0065\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0062\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0060\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0058\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0056\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0054\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0052\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0051\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.0049\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.0047\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ef3cb2730>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybvYRKc7hNgF",
        "outputId": "e44ef621-8a86-4df7-dd89-384a74253ff7"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his nephew mr henry dashwood the legal inheritor of the norland estate and the\n",
            "Seed word sequence: their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her loss he invited and received into his house the family of his nephew mr henry dashwood the legal inheritor of the norland estate and the\n",
            "Predicted words: person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children added relish to his existence by former marriage mr henry dashwood had one son by his present lady three daughters the son steady respectable young man was\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OdtLURYijpa"
      },
      "source": [
        "## 3. Normalisation does not always provide the best results. Remove normalisation and see if this improves the results (this will probably mean the model hyper-parameters also need changing).\n",
        "\n",
        "The model has learnt by 20 epochs, so not having normalisation has sped up the model training.  The number of neurons in the model can also be reduced but this increases the number of epochs it takes to train.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FGGjud3imPQ",
        "outputId": "2d0d46b6-9321-47b0-883e-e0244b4cc645"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "# X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=20, verbose=1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_22\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_22 (LSTM)              (None, 800)               2566400   \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,841,944\n",
            "Trainable params: 2,841,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "13/13 [==============================] - 3s 50ms/step - loss: 5.5168\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.9382\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.7529\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.6072\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.4260\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 4.2324\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 4.0008\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.7450\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 3.4495\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 1s 44ms/step - loss: 3.1318\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 2.7732\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 2.3991\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 2.0188\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 1.6334\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.3329\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 1.0559\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 1s 45ms/step - loss: 0.8334\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.6458\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.5082\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 1s 46ms/step - loss: 0.3945\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f04711550>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccz-a4g8imvh",
        "outputId": "2664cedf-267d-44bf-d22a-506e901a89bc"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the family of dashwood had long been settled in sussex their estate was large and their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her\n",
            "Seed word sequence: the family of dashwood had long been settled in sussex their estate was large and their residence was at norland park in the centre of their property where for many generations they had lived in so respectable manner as to engage the general good opinion of their surrounding acquaintance the late owner of this estate was single man who lived to very advanced age and who for many years of his life had constant companion and housekeeper in his sister but her death which happened ten years before his own produced great alteration in his home for to supply her\n",
            "Predicted words: loss he invited and received into his house the family of his nephew mr henry dashwood the legal inheritor of the norland estate and the person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children added relish\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCwKDdQEkLv0"
      },
      "source": [
        "## 4. Add an Embedding Layer into the DNN to see if this improves the model.\n",
        "\n",
        "Adding an embedding layer and removing normalisation allows the model to acheive good results after 35 epochs.\n",
        "\n",
        "The dimension of the dense embedding layer was arbitarily selected to be 50.  This is a parameter that can be experimented with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znTg1sDZkNPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "023b932e-436f-464d-bc71-b2eaa4500eb7"
      },
      "source": [
        "# import python libraries\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "import nltk   # natural language tool kit library\n",
        "nltk.download('gutenberg')  # downloads a library that NLTK uses\n",
        "\n",
        "from nltk.corpus import gutenberg as gut  # downloads the gutenberg dataset\n",
        "print(gut.fileids())    # prints the name of the files in the dataset\n",
        "\n",
        "# get the book text\n",
        "book_text = nltk.corpus.gutenberg.raw('austen-sense.txt')\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence.lower()\n",
        "book_text = preprocess_text(book_text)\n",
        "\n",
        "book_text = book_text[:5000]  # limit text to 5000, just for this exercise\n",
        "\n",
        "# convert words to numbers\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "book_text_words = (word_tokenize(book_text))\n",
        "n_words = len(book_text_words)\n",
        "unique_words = len(set(book_text_words))\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "tokenizer.fit_on_texts(book_text_words)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1    # word_index is the dictionary. Store the number of unique words in vocab_size variable\n",
        "word_2_index = tokenizer.word_index           # store the dictionary in the variable called word_2_index\n",
        "\n",
        "# Create the input sequences\n",
        "input_sequence_words = []  # input sequences in words (used for metric evaluation later on)\n",
        "input_sequence = []   # empty list to hold the sequences that will be input into our model\n",
        "output_words = []     # empty list to hold the output words\n",
        "input_seq_length = 100  # length of the input sequence\n",
        "for i in range(0, n_words - input_seq_length , 1):\n",
        "    in_seq = book_text_words[i:i + input_seq_length]\n",
        "    input_sequence_words.append(in_seq)\n",
        "    out_seq = book_text_words[i + input_seq_length]\n",
        "    input_sequence.append([word_2_index[word] for word in in_seq])\n",
        "    output_words.append(word_2_index[out_seq])\n",
        "\n",
        "# reshape the input sequences to be 3-dimensional\n",
        "X = np.reshape(input_sequence, (len(input_sequence), input_seq_length, 1))\n",
        "\n",
        "# Normalise the data by dividing by the max number of unique words (the vocab size)\n",
        "#X = X / float(vocab_size)\n",
        "\n",
        "# one-hot encode the output words so that they can be used by the model (converts the output to 2-dimensions)\n",
        "y = to_categorical(output_words)\n",
        "\n",
        "# create, compile and fit the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=X.shape[1]))\n",
        "model.add(LSTM(800, input_shape=(X.shape[1], X.shape[2]), return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "model.fit(X, y, batch_size=64, epochs=100, verbose=1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 100, 50)           17200     \n",
            "                                                                 \n",
            " lstm_23 (LSTM)              (None, 800)               2723200   \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 344)               275544    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,015,944\n",
            "Trainable params: 3,015,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 3s 108ms/step - loss: 5.6293\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 1s 65ms/step - loss: 5.2318\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 1s 51ms/step - loss: 5.1127\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 1s 53ms/step - loss: 5.0767\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 1s 53ms/step - loss: 5.0645\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 1s 59ms/step - loss: 5.0542\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 1s 61ms/step - loss: 5.0415\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 1s 61ms/step - loss: 5.0202\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 4.9946\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 1s 53ms/step - loss: 5.0181\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 4.9224\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 4.8021\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 4.6273\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 1s 85ms/step - loss: 4.4245\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 4.1145\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 3.7658\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 3.2968\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 2.9054\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 2.3573\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 1.7727\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 1.2758\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.8416\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 1s 56ms/step - loss: 0.5133\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.3027\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.2040\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.1281\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0928\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0685\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0539\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0462\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0382\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0336\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0295\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0271\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0243\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0225\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0208\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0194\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0180\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0169\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0159\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 1s 52ms/step - loss: 0.0149\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 1s 70ms/step - loss: 0.0141\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 1s 52ms/step - loss: 0.0133\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0127\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0120\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0114\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0109\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0104\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0100\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0095\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0091\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0087\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0084\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0081\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0078\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0075\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0072\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0069\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0067\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0065\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0063\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 1s 50ms/step - loss: 0.0061\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0059\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0057\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0055\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 1s 59ms/step - loss: 0.0053\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 1s 52ms/step - loss: 0.0052\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0050\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0049\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0047\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0046\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0045\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0044\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0042\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0041\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0040\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 1s 55ms/step - loss: 0.0039\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0038\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0037\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0036\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 1s 49ms/step - loss: 0.0035\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0035\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0034\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0033\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0032\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 1s 55ms/step - loss: 0.0031\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0031\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0030\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0029\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0029\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0028\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0028\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0027\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0026\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0026\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0025\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.0025\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0024\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 1s 48ms/step - loss: 0.0024\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ef5e399d0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE0y1mDYkNiG",
        "outputId": "6ae782fb-6e70-44a7-8f28-59e2ab812bc5"
      },
      "source": [
        "# Make Predictions\n",
        "random_seq_index = np.random.randint(0, len(input_sequence)-1)    # select a random number from within the range of the number of input sequences\n",
        "random_seq = input_sequence[random_seq_index]                     # get the input sequence that occurs at the randomly selected index (this is a list of integers)\n",
        "\n",
        "index_2_word = dict(map(reversed, word_2_index.items())) # convert the integer sequence to its words\n",
        "seed_word_sequence = [index_2_word[value] for value in random_seq]  # get the list of words that correspond to the integers in the randomly picked sequence\n",
        "\n",
        "# join the words in the list and print the sequence of words\n",
        "print(' '.join(seed_word_sequence))  # this prints the words from the randomly picked sequence that will be the seed for our prediction\n",
        "\n",
        "# Predict next 100 words\n",
        "word_sequence = []\n",
        "for i in range(100):\n",
        "    int_sample = np.reshape(random_seq, (1, len(random_seq), 1))    # reshape to make 3-D input (1 sequence, length of the sequence, 1 because the first LSTM requires another dimension)\n",
        "    # int_sample = int_sample / float(vocab_size)                     # normalise (as we normalised the training data)\n",
        "\n",
        "    predicted_word_index = model.predict(int_sample, verbose=0)     # predict the next word.  An array of the probabilities for each word in the vocab is returned.\n",
        "    predicted_word_id = np.argmax(predicted_word_index)             # get the index of the maximum value (they are categorical so the max value gives the word in the vocab with the highest probability)\n",
        "    word_sequence.append(index_2_word[ predicted_word_id])          # get the predicted word by finding the word at the predicted index and add it to our predicted word sequence list\n",
        "\n",
        "    random_seq.append(predicted_word_id)                            # append the predicted word index to the next seuqence to be input into the model predict method\n",
        "    random_seq = random_seq[1:len(random_seq)]                      # remove the first element of the sequence so it now has the new word but is the same length.\n",
        "\n",
        "# BLEU score\n",
        "seq = [' '.join(w) for w in input_sequence_words]\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "reference = seq\n",
        "candidate = ' '.join(word_sequence) # make the list of words into a string\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print('Seed word sequence: %s'%(' '.join(seed_word_sequence)))\n",
        "print('Predicted words: %s'%(candidate))\n",
        "print('BLEU Score for predicted words: %s'%(score))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "supply her loss he invited and received into his house the family of his nephew mr henry dashwood the legal inheritor of the norland estate and the person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children\n",
            "Seed word sequence: supply her loss he invited and received into his house the family of his nephew mr henry dashwood the legal inheritor of the norland estate and the person to whom he intended to bequeath it in the society of his nephew and niece and their children the old gentleman days were comfortably spent his attachment to them all increased the constant attention of mr and mrs henry dashwood to his wishes which proceeded not merely from interest but from goodness of heart gave him every degree of solid comfort which his age could receive and the cheerfulness of the children\n",
            "Predicted words: added relish to his existence by former marriage mr henry dashwood had one son by his present lady three daughters the son steady respectable young man was amply provided for by the fortune of his mother which had been large and half of which devolved on him on his coming of age by his own marriage likewise which happened soon afterwards he added to his wealth to him therefore the succession to the norland estate was not so really important as to his sisters for their fortune independent of what might arise to them from their father inheriting that property\n",
            "BLEU Score for predicted words: 1.0\n"
          ]
        }
      ]
    }
  ]
}