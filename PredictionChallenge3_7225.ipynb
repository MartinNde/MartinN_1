{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinNde/MartinN_1/blob/main/PredictionChallenge3_7225.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Prediction Challenge 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deGfOT2xuFgD",
        "outputId": "72457f80-d376-417a-9bc3-dac93dfaa04a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (67.6.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.27.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.1.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrUzv72bwiYP",
        "outputId": "43474282-1c30-407c-e513-77fc886f6253"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.1.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WMz2eOtwo3v",
        "outputId": "6d7d9fbf-7aa7-4e37-abd0-56a3a461d167"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "# here the sequential memory limit is set up the same as the nb_steps (number of steps)\n",
        "# parameter in the fit method.  This means that all the action-states will fit into the\n",
        "# memory buffer\n",
        "# keep window_length as 1. It's used in other RL methods, but keep it to 1 in DQNs\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "policy = BoltzmannQPolicy()"
      ],
      "metadata": {
        "id": "aFPJ_maZwsf_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ],
      "metadata": {
        "id": "uNNdoXo4w_CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# setup the Linear annealed policy with the BoltzmannQPolicy as the inner policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(),   # policy used to select actions\n",
        "                               attr='eps',                        # attribute in the inner policy to vary             \n",
        "                               value_max=1,                       # maximum value of attribute that is varying\n",
        "                               value_min=0.1,                      # minimum value of attribute that is varying\n",
        "                               value_test=0.05,                    # test if the value selected is < 0.05\n",
        "                               nb_steps=1000)                    # the number of steps between value_max and value_min\n",
        "\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\n",
        "               nb_actions=env.action_space.n,   # number of actions\n",
        "               memory=memory,                   # experience replay memory\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\n",
        "               target_model_update=1e-2,        # how often the target network is updated\n",
        "               policy=policy)                   # the action selection policy\n",
        "\n",
        "dqn.compile(Adam(lr=1e-2), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F1yI7Cuxwxov",
        "outputId": "e0bcf3d0-76c6-44e6-f703-f1b198bd59b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_8 (Flatten)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   20/10000: episode: 1, duration: 2.050s, episode steps:  20, steps per second:  10, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.350 [0.000, 1.000],  loss: 0.450389, mae: 0.561566, mean_q: 0.270058, mean_eps: 0.986500\n",
            "   31/10000: episode: 2, duration: 0.091s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.155565, mae: 0.603085, mean_q: 0.876197, mean_eps: 0.977500\n",
            "   54/10000: episode: 3, duration: 0.189s, episode steps:  23, steps per second: 122, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.060333, mae: 0.646667, mean_q: 1.149004, mean_eps: 0.962200\n",
            "   72/10000: episode: 4, duration: 0.139s, episode steps:  18, steps per second: 130, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.010928, mae: 0.731803, mean_q: 1.537835, mean_eps: 0.943750\n",
            "  102/10000: episode: 5, duration: 0.219s, episode steps:  30, steps per second: 137, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.008152, mae: 0.794030, mean_q: 1.631205, mean_eps: 0.922150\n",
            "  114/10000: episode: 6, duration: 0.089s, episode steps:  12, steps per second: 135, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.012598, mae: 0.888259, mean_q: 1.791903, mean_eps: 0.903250\n",
            "  142/10000: episode: 7, duration: 0.211s, episode steps:  28, steps per second: 133, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.014709, mae: 0.964276, mean_q: 1.963206, mean_eps: 0.885250\n",
            "  153/10000: episode: 8, duration: 0.089s, episode steps:  11, steps per second: 123, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.017553, mae: 1.053727, mean_q: 2.107334, mean_eps: 0.867700\n",
            "  172/10000: episode: 9, duration: 0.149s, episode steps:  19, steps per second: 128, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.025263, mae: 1.111641, mean_q: 2.212147, mean_eps: 0.854200\n",
            "  204/10000: episode: 10, duration: 0.250s, episode steps:  32, steps per second: 128, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.035336, mae: 1.222839, mean_q: 2.425954, mean_eps: 0.831250\n",
            "  221/10000: episode: 11, duration: 0.127s, episode steps:  17, steps per second: 134, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.037439, mae: 1.322970, mean_q: 2.681378, mean_eps: 0.809200\n",
            "  238/10000: episode: 12, duration: 0.136s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.049984, mae: 1.438483, mean_q: 2.848384, mean_eps: 0.793900\n",
            "  263/10000: episode: 13, duration: 0.179s, episode steps:  25, steps per second: 140, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.075659, mae: 1.557779, mean_q: 3.061573, mean_eps: 0.775000\n",
            "  286/10000: episode: 14, duration: 0.171s, episode steps:  23, steps per second: 135, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.391 [0.000, 1.000],  loss: 0.087738, mae: 1.672884, mean_q: 3.328912, mean_eps: 0.753400\n",
            "  302/10000: episode: 15, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.312 [0.000, 1.000],  loss: 0.091559, mae: 1.763542, mean_q: 3.413291, mean_eps: 0.735850\n",
            "  311/10000: episode: 16, duration: 0.076s, episode steps:   9, steps per second: 118, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.139450, mae: 1.839749, mean_q: 3.600912, mean_eps: 0.724600\n",
            "  326/10000: episode: 17, duration: 0.124s, episode steps:  15, steps per second: 121, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.173686, mae: 1.924039, mean_q: 3.703273, mean_eps: 0.713800\n",
            "  351/10000: episode: 18, duration: 0.205s, episode steps:  25, steps per second: 122, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.320 [0.000, 1.000],  loss: 0.208504, mae: 2.016112, mean_q: 3.884386, mean_eps: 0.695800\n",
            "  364/10000: episode: 19, duration: 0.102s, episode steps:  13, steps per second: 128, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.157636, mae: 2.095326, mean_q: 4.082191, mean_eps: 0.678700\n",
            "  379/10000: episode: 20, duration: 0.113s, episode steps:  15, steps per second: 132, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.234450, mae: 2.172529, mean_q: 4.120750, mean_eps: 0.666100\n",
            "  400/10000: episode: 21, duration: 0.152s, episode steps:  21, steps per second: 138, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.236602, mae: 2.274236, mean_q: 4.328151, mean_eps: 0.649900\n",
            "  408/10000: episode: 22, duration: 0.065s, episode steps:   8, steps per second: 123, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.266695, mae: 2.300251, mean_q: 4.404985, mean_eps: 0.636850\n",
            "  419/10000: episode: 23, duration: 0.084s, episode steps:  11, steps per second: 131, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.287765, mae: 2.352512, mean_q: 4.375803, mean_eps: 0.628300\n",
            "  440/10000: episode: 24, duration: 0.189s, episode steps:  21, steps per second: 111, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.271287, mae: 2.420102, mean_q: 4.639867, mean_eps: 0.613900\n",
            "  457/10000: episode: 25, duration: 0.198s, episode steps:  17, steps per second:  86, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.214670, mae: 2.486947, mean_q: 4.728553, mean_eps: 0.596800\n",
            "  475/10000: episode: 26, duration: 0.193s, episode steps:  18, steps per second:  93, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.389 [0.000, 1.000],  loss: 0.200683, mae: 2.591397, mean_q: 4.969596, mean_eps: 0.581050\n",
            "  495/10000: episode: 27, duration: 0.205s, episode steps:  20, steps per second:  98, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.278696, mae: 2.654510, mean_q: 4.993035, mean_eps: 0.563950\n",
            "  522/10000: episode: 28, duration: 0.294s, episode steps:  27, steps per second:  92, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.184828, mae: 2.758355, mean_q: 5.300609, mean_eps: 0.542800\n",
            "  546/10000: episode: 29, duration: 0.260s, episode steps:  24, steps per second:  92, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.231865, mae: 2.859182, mean_q: 5.528972, mean_eps: 0.519850\n",
            "  582/10000: episode: 30, duration: 0.381s, episode steps:  36, steps per second:  94, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 0.287797, mae: 2.966656, mean_q: 5.632797, mean_eps: 0.492850\n",
            "  596/10000: episode: 31, duration: 0.171s, episode steps:  14, steps per second:  82, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.320109, mae: 3.088432, mean_q: 5.923891, mean_eps: 0.470350\n",
            "  675/10000: episode: 32, duration: 0.844s, episode steps:  79, steps per second:  94, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.225858, mae: 3.185299, mean_q: 6.195337, mean_eps: 0.428500\n",
            "  822/10000: episode: 33, duration: 1.088s, episode steps: 147, steps per second: 135, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.265751, mae: 3.548605, mean_q: 6.990348, mean_eps: 0.326800\n",
            "  845/10000: episode: 34, duration: 0.171s, episode steps:  23, steps per second: 135, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.235408, mae: 3.884949, mean_q: 7.716551, mean_eps: 0.250300\n",
            "  873/10000: episode: 35, duration: 0.215s, episode steps:  28, steps per second: 130, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.180316, mae: 3.958127, mean_q: 7.924747, mean_eps: 0.227350\n",
            " 1030/10000: episode: 36, duration: 1.109s, episode steps: 157, steps per second: 142, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.312214, mae: 4.391779, mean_q: 8.718102, mean_eps: 0.146594\n",
            " 1128/10000: episode: 37, duration: 0.686s, episode steps:  98, steps per second: 143, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.410014, mae: 4.837859, mean_q: 9.594111, mean_eps: 0.100000\n",
            " 1296/10000: episode: 38, duration: 1.212s, episode steps: 168, steps per second: 139, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.316240, mae: 5.349713, mean_q: 10.682084, mean_eps: 0.100000\n",
            " 1397/10000: episode: 39, duration: 0.723s, episode steps: 101, steps per second: 140, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.381336, mae: 5.838846, mean_q: 11.712628, mean_eps: 0.100000\n",
            " 1491/10000: episode: 40, duration: 0.708s, episode steps:  94, steps per second: 133, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.421692, mae: 6.171789, mean_q: 12.364799, mean_eps: 0.100000\n",
            " 1573/10000: episode: 41, duration: 0.625s, episode steps:  82, steps per second: 131, episode reward: 82.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 0.476264, mae: 6.500423, mean_q: 13.011142, mean_eps: 0.100000\n",
            " 1684/10000: episode: 42, duration: 0.823s, episode steps: 111, steps per second: 135, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.521272, mae: 6.855184, mean_q: 13.729564, mean_eps: 0.100000\n",
            " 1802/10000: episode: 43, duration: 0.849s, episode steps: 118, steps per second: 139, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.424509, mae: 7.324459, mean_q: 14.679792, mean_eps: 0.100000\n",
            " 1890/10000: episode: 44, duration: 0.629s, episode steps:  88, steps per second: 140, episode reward: 88.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.641729, mae: 7.709885, mean_q: 15.392251, mean_eps: 0.100000\n",
            " 1968/10000: episode: 45, duration: 0.577s, episode steps:  78, steps per second: 135, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.617649, mae: 8.027507, mean_q: 16.084273, mean_eps: 0.100000\n",
            " 2066/10000: episode: 46, duration: 0.762s, episode steps:  98, steps per second: 129, episode reward: 98.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.608549, mae: 8.395428, mean_q: 16.775411, mean_eps: 0.100000\n",
            " 2155/10000: episode: 47, duration: 0.917s, episode steps:  89, steps per second:  97, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 0.581873, mae: 8.643566, mean_q: 17.355561, mean_eps: 0.100000\n",
            " 2245/10000: episode: 48, duration: 0.904s, episode steps:  90, steps per second: 100, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.791479, mae: 9.065671, mean_q: 18.083062, mean_eps: 0.100000\n",
            " 2404/10000: episode: 49, duration: 1.329s, episode steps: 159, steps per second: 120, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.756699, mae: 9.460290, mean_q: 18.915895, mean_eps: 0.100000\n",
            " 2509/10000: episode: 50, duration: 0.765s, episode steps: 105, steps per second: 137, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.883216, mae: 9.885887, mean_q: 19.722620, mean_eps: 0.100000\n",
            " 2667/10000: episode: 51, duration: 1.149s, episode steps: 158, steps per second: 138, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.982536, mae: 10.340731, mean_q: 20.632600, mean_eps: 0.100000\n",
            " 2756/10000: episode: 52, duration: 0.647s, episode steps:  89, steps per second: 138, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.551 [0.000, 1.000],  loss: 0.967760, mae: 10.660694, mean_q: 21.276009, mean_eps: 0.100000\n",
            " 2874/10000: episode: 53, duration: 0.883s, episode steps: 118, steps per second: 134, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.823479, mae: 10.998807, mean_q: 21.999671, mean_eps: 0.100000\n",
            " 2977/10000: episode: 54, duration: 0.743s, episode steps: 103, steps per second: 139, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.926375, mae: 11.406127, mean_q: 22.818521, mean_eps: 0.100000\n",
            " 3105/10000: episode: 55, duration: 0.917s, episode steps: 128, steps per second: 140, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.886192, mae: 11.682726, mean_q: 23.375959, mean_eps: 0.100000\n",
            " 3243/10000: episode: 56, duration: 1.020s, episode steps: 138, steps per second: 135, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.932675, mae: 12.193485, mean_q: 24.430659, mean_eps: 0.100000\n",
            " 3350/10000: episode: 57, duration: 0.798s, episode steps: 107, steps per second: 134, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.895243, mae: 12.494325, mean_q: 25.039581, mean_eps: 0.100000\n",
            " 3471/10000: episode: 58, duration: 0.880s, episode steps: 121, steps per second: 138, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 0.873863, mae: 12.835872, mean_q: 25.722514, mean_eps: 0.100000\n",
            " 3566/10000: episode: 59, duration: 0.688s, episode steps:  95, steps per second: 138, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.097935, mae: 13.176052, mean_q: 26.403977, mean_eps: 0.100000\n",
            " 3682/10000: episode: 60, duration: 0.883s, episode steps: 116, steps per second: 131, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.874794, mae: 13.518551, mean_q: 27.062282, mean_eps: 0.100000\n",
            " 3768/10000: episode: 61, duration: 0.892s, episode steps:  86, steps per second:  96, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.402711, mae: 13.560434, mean_q: 27.067088, mean_eps: 0.100000\n",
            " 3869/10000: episode: 62, duration: 1.035s, episode steps: 101, steps per second:  98, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.805906, mae: 13.819412, mean_q: 27.667933, mean_eps: 0.100000\n",
            " 3993/10000: episode: 63, duration: 1.101s, episode steps: 124, steps per second: 113, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.782229, mae: 14.180922, mean_q: 28.471978, mean_eps: 0.100000\n",
            " 4106/10000: episode: 64, duration: 0.812s, episode steps: 113, steps per second: 139, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 0.892474, mae: 14.300365, mean_q: 28.703973, mean_eps: 0.100000\n",
            " 4227/10000: episode: 65, duration: 0.894s, episode steps: 121, steps per second: 135, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.884280, mae: 14.620680, mean_q: 29.336151, mean_eps: 0.100000\n",
            " 4337/10000: episode: 66, duration: 0.795s, episode steps: 110, steps per second: 138, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.970513, mae: 14.964501, mean_q: 30.052370, mean_eps: 0.100000\n",
            " 4448/10000: episode: 67, duration: 0.819s, episode steps: 111, steps per second: 135, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.542231, mae: 15.170595, mean_q: 30.382501, mean_eps: 0.100000\n",
            " 4561/10000: episode: 68, duration: 0.846s, episode steps: 113, steps per second: 134, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.435848, mae: 15.259545, mean_q: 30.451458, mean_eps: 0.100000\n",
            " 4670/10000: episode: 69, duration: 0.794s, episode steps: 109, steps per second: 137, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.854715, mae: 15.647577, mean_q: 31.484049, mean_eps: 0.100000\n",
            " 4785/10000: episode: 70, duration: 0.819s, episode steps: 115, steps per second: 140, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.041246, mae: 15.816059, mean_q: 31.730107, mean_eps: 0.100000\n",
            " 4900/10000: episode: 71, duration: 0.851s, episode steps: 115, steps per second: 135, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 0.886360, mae: 15.936716, mean_q: 31.993910, mean_eps: 0.100000\n",
            " 5029/10000: episode: 72, duration: 0.951s, episode steps: 129, steps per second: 136, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.828437, mae: 16.176656, mean_q: 32.483116, mean_eps: 0.100000\n",
            " 5167/10000: episode: 73, duration: 0.983s, episode steps: 138, steps per second: 140, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.261072, mae: 16.246959, mean_q: 32.584508, mean_eps: 0.100000\n",
            " 5324/10000: episode: 74, duration: 1.248s, episode steps: 157, steps per second: 126, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.370510, mae: 16.801997, mean_q: 33.617680, mean_eps: 0.100000\n",
            " 5482/10000: episode: 75, duration: 1.587s, episode steps: 158, steps per second: 100, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.010942, mae: 16.991890, mean_q: 34.081828, mean_eps: 0.100000\n",
            " 5628/10000: episode: 76, duration: 1.242s, episode steps: 146, steps per second: 118, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.848871, mae: 17.107407, mean_q: 34.462675, mean_eps: 0.100000\n",
            " 5768/10000: episode: 77, duration: 0.995s, episode steps: 140, steps per second: 141, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.059376, mae: 17.519398, mean_q: 35.165588, mean_eps: 0.100000\n",
            " 5853/10000: episode: 78, duration: 0.610s, episode steps:  85, steps per second: 139, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.468742, mae: 17.611495, mean_q: 35.326386, mean_eps: 0.100000\n",
            " 6001/10000: episode: 79, duration: 1.061s, episode steps: 148, steps per second: 139, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.078904, mae: 18.057418, mean_q: 36.207134, mean_eps: 0.100000\n",
            " 6148/10000: episode: 80, duration: 1.087s, episode steps: 147, steps per second: 135, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.129481, mae: 18.122461, mean_q: 36.356646, mean_eps: 0.100000\n",
            " 6335/10000: episode: 81, duration: 1.333s, episode steps: 187, steps per second: 140, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 1.257266, mae: 18.455555, mean_q: 36.984338, mean_eps: 0.100000\n",
            " 6535/10000: episode: 82, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.143802, mae: 18.661004, mean_q: 37.416102, mean_eps: 0.100000\n",
            " 6629/10000: episode: 83, duration: 0.643s, episode steps:  94, steps per second: 146, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.110266, mae: 18.735232, mean_q: 37.541709, mean_eps: 0.100000\n",
            " 6829/10000: episode: 84, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 0.939778, mae: 19.257985, mean_q: 38.691793, mean_eps: 0.100000\n",
            " 7029/10000: episode: 85, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.147951, mae: 19.762949, mean_q: 39.760486, mean_eps: 0.100000\n",
            " 7229/10000: episode: 86, duration: 1.939s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.559715, mae: 20.268898, mean_q: 40.701216, mean_eps: 0.100000\n",
            " 7429/10000: episode: 87, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.468824, mae: 20.821583, mean_q: 41.961757, mean_eps: 0.100000\n",
            " 7629/10000: episode: 88, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.369492, mae: 21.442791, mean_q: 43.123368, mean_eps: 0.100000\n",
            " 7829/10000: episode: 89, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.485109, mae: 21.975995, mean_q: 44.164097, mean_eps: 0.100000\n",
            " 8029/10000: episode: 90, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.951931, mae: 22.637523, mean_q: 45.563053, mean_eps: 0.100000\n",
            " 8229/10000: episode: 91, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.100139, mae: 23.293168, mean_q: 47.084617, mean_eps: 0.100000\n",
            " 8415/10000: episode: 92, duration: 1.309s, episode steps: 186, steps per second: 142, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.853436, mae: 23.994445, mean_q: 48.348965, mean_eps: 0.100000\n",
            " 8613/10000: episode: 93, duration: 1.554s, episode steps: 198, steps per second: 127, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 5.243756, mae: 24.794815, mean_q: 49.794536, mean_eps: 0.100000\n",
            " 8781/10000: episode: 94, duration: 1.760s, episode steps: 168, steps per second:  95, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.884360, mae: 25.675504, mean_q: 51.694667, mean_eps: 0.100000\n",
            " 8981/10000: episode: 95, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.104685, mae: 26.564431, mean_q: 53.380483, mean_eps: 0.100000\n",
            " 9181/10000: episode: 96, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.180838, mae: 27.403353, mean_q: 55.122974, mean_eps: 0.100000\n",
            " 9381/10000: episode: 97, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.164232, mae: 28.336167, mean_q: 57.004629, mean_eps: 0.100000\n",
            " 9581/10000: episode: 98, duration: 1.395s, episode steps: 200, steps per second: 143, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 6.259041, mae: 29.149358, mean_q: 58.679329, mean_eps: 0.100000\n",
            " 9781/10000: episode: 99, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.133997, mae: 30.312437, mean_q: 61.137029, mean_eps: 0.100000\n",
            " 9981/10000: episode: 100, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 9.216721, mae: 31.456381, mean_q: 63.224543, mean_eps: 0.100000\n",
            "done, took 79.134 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4ef1b68670>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes I made :\n",
        "\n",
        "I changed the learning rate to 1e-2 and nub_step to 1000 - b/c a slower annealing schdedule can help the agent explore more thorougly, leading to a better performance. "
      ],
      "metadata": {
        "id": "as4eO5iZUp6W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZJur1Q5VawQ"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}