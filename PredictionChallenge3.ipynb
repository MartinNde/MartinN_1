{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PredictionChallenge3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinNde/MartinN_1/blob/main/PredictionChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Prediction Challenge 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D"
      },
      "source": [],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deGfOT2xuFgD",
        "outputId": "dc8d8bb9-5326-4167-d9fa-1d12328d6082"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.9/dist-packages (1.0.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (63.4.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.38.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.25.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrUzv72bwiYP",
        "outputId": "6514247d-1177-4070-d40d-993c5bc9c875"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WMz2eOtwo3v",
        "outputId": "9e7be2ce-92d9-467d-81da-ca8d22a0daab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "# here the sequential memory limit is set up the same as the nb_steps (number of steps)\n",
        "# parameter in the fit method.  This means that all the action-states will fit into the\n",
        "# memory buffer\n",
        "# keep window_length as 1. It's used in other RL methods, but keep it to 1 in DQNs\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "policy = BoltzmannQPolicy()"
      ],
      "metadata": {
        "id": "aFPJ_maZwsf_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ],
      "metadata": {
        "id": "uNNdoXo4w_CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# setup the Linear annealed policy with the BoltzmannQPolicy as the inner policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(),   # policy used to select actions\n",
        "                               attr='eps',                        # attribute in the inner policy to vary             \n",
        "                               value_max=1,                       # maximum value of attribute that is varying\n",
        "                               value_min=0.1,                      # minimum value of attribute that is varying\n",
        "                               value_test=0.05,                    # test if the value selected is < 0.05\n",
        "                               nb_steps=10000)                    # the number of steps between value_max and value_min\n",
        "\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\n",
        "               nb_actions=env.action_space.n,   # number of actions\n",
        "               memory=memory,                   # experience replay memory\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\n",
        "               target_model_update=1e-2,        # how often the target network is updated\n",
        "               policy=policy)                   # the action selection policy\n",
        "\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F1yI7Cuxwxov",
        "outputId": "85af3414-339f-4472-8314-09d1d19495b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                80        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   15/10000: episode: 1, duration: 0.660s, episode steps:  15, steps per second:  23, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.534447, mae: 0.561791, mean_q: -0.047805, mean_eps: 0.998875\n",
            "   39/10000: episode: 2, duration: 0.149s, episode steps:  24, steps per second: 161, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.489813, mae: 0.518800, mean_q: 0.045338, mean_eps: 0.997615\n",
            "  109/10000: episode: 3, duration: 0.417s, episode steps:  70, steps per second: 168, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.366246, mae: 0.548305, mean_q: 0.280911, mean_eps: 0.993385\n",
            "  123/10000: episode: 4, duration: 0.083s, episode steps:  14, steps per second: 168, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.261215, mae: 0.614135, mean_q: 0.563485, mean_eps: 0.989605\n",
            "  152/10000: episode: 5, duration: 0.173s, episode steps:  29, steps per second: 167, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.202969, mae: 0.642840, mean_q: 0.727066, mean_eps: 0.987670\n",
            "  166/10000: episode: 6, duration: 0.096s, episode steps:  14, steps per second: 146, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.186312, mae: 0.669614, mean_q: 0.825831, mean_eps: 0.985735\n",
            "  193/10000: episode: 7, duration: 0.165s, episode steps:  27, steps per second: 164, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.162997, mae: 0.716024, mean_q: 0.963001, mean_eps: 0.983890\n",
            "  207/10000: episode: 8, duration: 0.089s, episode steps:  14, steps per second: 157, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.151548, mae: 0.757435, mean_q: 1.066039, mean_eps: 0.982045\n",
            "  224/10000: episode: 9, duration: 0.106s, episode steps:  17, steps per second: 160, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.137270, mae: 0.808889, mean_q: 1.218488, mean_eps: 0.980650\n",
            "  255/10000: episode: 10, duration: 0.184s, episode steps:  31, steps per second: 168, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.387 [0.000, 1.000],  loss: 0.129521, mae: 0.862498, mean_q: 1.341533, mean_eps: 0.978490\n",
            "  287/10000: episode: 11, duration: 0.193s, episode steps:  32, steps per second: 166, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.116623, mae: 0.947452, mean_q: 1.553263, mean_eps: 0.975655\n",
            "  306/10000: episode: 12, duration: 0.128s, episode steps:  19, steps per second: 148, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.684 [0.000, 1.000],  loss: 0.112499, mae: 1.055780, mean_q: 1.802772, mean_eps: 0.973360\n",
            "  337/10000: episode: 13, duration: 0.210s, episode steps:  31, steps per second: 148, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.419 [0.000, 1.000],  loss: 0.125661, mae: 1.128546, mean_q: 1.958141, mean_eps: 0.971110\n",
            "  345/10000: episode: 14, duration: 0.052s, episode steps:   8, steps per second: 155, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.115861, mae: 1.193170, mean_q: 2.118823, mean_eps: 0.969355\n",
            "  374/10000: episode: 15, duration: 0.173s, episode steps:  29, steps per second: 168, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.132482, mae: 1.265496, mean_q: 2.244196, mean_eps: 0.967690\n",
            "  393/10000: episode: 16, duration: 0.111s, episode steps:  19, steps per second: 171, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.151959, mae: 1.377881, mean_q: 2.485701, mean_eps: 0.965530\n",
            "  426/10000: episode: 17, duration: 0.198s, episode steps:  33, steps per second: 167, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.147990, mae: 1.450638, mean_q: 2.643525, mean_eps: 0.963190\n",
            "  475/10000: episode: 18, duration: 0.287s, episode steps:  49, steps per second: 171, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.171291, mae: 1.615863, mean_q: 3.005824, mean_eps: 0.959500\n",
            "  490/10000: episode: 19, duration: 0.090s, episode steps:  15, steps per second: 167, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.218098, mae: 1.785391, mean_q: 3.304675, mean_eps: 0.956620\n",
            "  527/10000: episode: 20, duration: 0.229s, episode steps:  37, steps per second: 161, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.211840, mae: 1.846194, mean_q: 3.468652, mean_eps: 0.954280\n",
            "  562/10000: episode: 21, duration: 0.206s, episode steps:  35, steps per second: 170, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.221531, mae: 2.015040, mean_q: 3.835499, mean_eps: 0.951040\n",
            "  573/10000: episode: 22, duration: 0.069s, episode steps:  11, steps per second: 158, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.179353, mae: 2.050389, mean_q: 3.933492, mean_eps: 0.948970\n",
            "  614/10000: episode: 23, duration: 0.255s, episode steps:  41, steps per second: 161, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.280697, mae: 2.200324, mean_q: 4.150886, mean_eps: 0.946630\n",
            "  635/10000: episode: 24, duration: 0.121s, episode steps:  21, steps per second: 174, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 0.374169, mae: 2.349074, mean_q: 4.430366, mean_eps: 0.943840\n",
            "  659/10000: episode: 25, duration: 0.154s, episode steps:  24, steps per second: 156, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.248349, mae: 2.405249, mean_q: 4.630667, mean_eps: 0.941815\n",
            "  682/10000: episode: 26, duration: 0.135s, episode steps:  23, steps per second: 171, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.652 [0.000, 1.000],  loss: 0.317428, mae: 2.531748, mean_q: 4.886549, mean_eps: 0.939700\n",
            "  701/10000: episode: 27, duration: 0.119s, episode steps:  19, steps per second: 160, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.544347, mae: 2.649596, mean_q: 5.050737, mean_eps: 0.937810\n",
            "  735/10000: episode: 28, duration: 0.292s, episode steps:  34, steps per second: 117, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.382410, mae: 2.737737, mean_q: 5.303242, mean_eps: 0.935425\n",
            "  759/10000: episode: 29, duration: 0.199s, episode steps:  24, steps per second: 121, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.344402, mae: 2.862505, mean_q: 5.523974, mean_eps: 0.932815\n",
            "  783/10000: episode: 30, duration: 0.212s, episode steps:  24, steps per second: 113, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.708 [0.000, 1.000],  loss: 0.556978, mae: 2.991440, mean_q: 5.728738, mean_eps: 0.930655\n",
            "  795/10000: episode: 31, duration: 0.120s, episode steps:  12, steps per second: 100, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.452789, mae: 3.053661, mean_q: 5.848527, mean_eps: 0.929035\n",
            "  815/10000: episode: 32, duration: 0.179s, episode steps:  20, steps per second: 111, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.537549, mae: 3.126623, mean_q: 5.952026, mean_eps: 0.927595\n",
            "  835/10000: episode: 33, duration: 0.166s, episode steps:  20, steps per second: 120, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.604285, mae: 3.208955, mean_q: 6.171467, mean_eps: 0.925795\n",
            "  866/10000: episode: 34, duration: 0.263s, episode steps:  31, steps per second: 118, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.613 [0.000, 1.000],  loss: 0.770871, mae: 3.358875, mean_q: 6.355442, mean_eps: 0.923500\n",
            "  874/10000: episode: 35, duration: 0.082s, episode steps:   8, steps per second:  98, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.606483, mae: 3.421571, mean_q: 6.498415, mean_eps: 0.921745\n",
            "  892/10000: episode: 36, duration: 0.157s, episode steps:  18, steps per second: 115, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.517733, mae: 3.429631, mean_q: 6.607951, mean_eps: 0.920575\n",
            "  911/10000: episode: 37, duration: 0.162s, episode steps:  19, steps per second: 117, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.316 [0.000, 1.000],  loss: 0.873393, mae: 3.557167, mean_q: 6.755410, mean_eps: 0.918910\n",
            "  928/10000: episode: 38, duration: 0.153s, episode steps:  17, steps per second: 111, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.658485, mae: 3.601945, mean_q: 6.835270, mean_eps: 0.917290\n",
            "  938/10000: episode: 39, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.429329, mae: 3.576743, mean_q: 6.959540, mean_eps: 0.916075\n",
            "  963/10000: episode: 40, duration: 0.218s, episode steps:  25, steps per second: 115, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 0.744058, mae: 3.738814, mean_q: 7.214800, mean_eps: 0.914500\n",
            "  989/10000: episode: 41, duration: 0.227s, episode steps:  26, steps per second: 115, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.858450, mae: 3.832139, mean_q: 7.211462, mean_eps: 0.912205\n",
            " 1012/10000: episode: 42, duration: 0.176s, episode steps:  23, steps per second: 131, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.736278, mae: 3.909569, mean_q: 7.459362, mean_eps: 0.910000\n",
            " 1026/10000: episode: 43, duration: 0.088s, episode steps:  14, steps per second: 159, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.852312, mae: 3.983677, mean_q: 7.654755, mean_eps: 0.908335\n",
            " 1045/10000: episode: 44, duration: 0.117s, episode steps:  19, steps per second: 162, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.840794, mae: 4.049106, mean_q: 7.699687, mean_eps: 0.906850\n",
            " 1066/10000: episode: 45, duration: 0.133s, episode steps:  21, steps per second: 158, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.875894, mae: 4.122119, mean_q: 7.816924, mean_eps: 0.905050\n",
            " 1082/10000: episode: 46, duration: 0.097s, episode steps:  16, steps per second: 164, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.645670, mae: 4.133937, mean_q: 7.904676, mean_eps: 0.903385\n",
            " 1097/10000: episode: 47, duration: 0.090s, episode steps:  15, steps per second: 167, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.702621, mae: 4.186261, mean_q: 8.031470, mean_eps: 0.901990\n",
            " 1118/10000: episode: 48, duration: 0.134s, episode steps:  21, steps per second: 157, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.619 [0.000, 1.000],  loss: 1.056559, mae: 4.304671, mean_q: 8.176178, mean_eps: 0.900370\n",
            " 1168/10000: episode: 49, duration: 0.302s, episode steps:  50, steps per second: 165, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 1.047476, mae: 4.410332, mean_q: 8.266816, mean_eps: 0.897175\n",
            " 1213/10000: episode: 50, duration: 0.271s, episode steps:  45, steps per second: 166, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 1.095360, mae: 4.558824, mean_q: 8.514651, mean_eps: 0.892900\n",
            " 1230/10000: episode: 51, duration: 0.102s, episode steps:  17, steps per second: 167, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.767340, mae: 4.601993, mean_q: 8.754797, mean_eps: 0.890110\n",
            " 1243/10000: episode: 52, duration: 0.078s, episode steps:  13, steps per second: 166, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.883108, mae: 4.657674, mean_q: 8.886101, mean_eps: 0.888760\n",
            " 1279/10000: episode: 53, duration: 0.218s, episode steps:  36, steps per second: 165, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.046905, mae: 4.751254, mean_q: 9.041409, mean_eps: 0.886555\n",
            " 1292/10000: episode: 54, duration: 0.078s, episode steps:  13, steps per second: 166, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 1.125633, mae: 4.803686, mean_q: 9.020346, mean_eps: 0.884350\n",
            " 1324/10000: episode: 55, duration: 0.194s, episode steps:  32, steps per second: 165, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.958621, mae: 4.888379, mean_q: 9.301989, mean_eps: 0.882325\n",
            " 1340/10000: episode: 56, duration: 0.107s, episode steps:  16, steps per second: 149, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 1.393305, mae: 5.024558, mean_q: 9.461295, mean_eps: 0.880165\n",
            " 1367/10000: episode: 57, duration: 0.165s, episode steps:  27, steps per second: 164, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.748560, mae: 5.108202, mean_q: 9.492287, mean_eps: 0.878230\n",
            " 1378/10000: episode: 58, duration: 0.075s, episode steps:  11, steps per second: 147, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.364346, mae: 5.117739, mean_q: 9.490912, mean_eps: 0.876520\n",
            " 1388/10000: episode: 59, duration: 0.068s, episode steps:  10, steps per second: 148, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 1.743722, mae: 5.122806, mean_q: 9.441102, mean_eps: 0.875575\n",
            " 1417/10000: episode: 60, duration: 0.176s, episode steps:  29, steps per second: 165, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.414 [0.000, 1.000],  loss: 1.326802, mae: 5.154890, mean_q: 9.693624, mean_eps: 0.873820\n",
            " 1438/10000: episode: 61, duration: 0.130s, episode steps:  21, steps per second: 161, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.392127, mae: 5.233643, mean_q: 9.792953, mean_eps: 0.871570\n",
            " 1462/10000: episode: 62, duration: 0.147s, episode steps:  24, steps per second: 164, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 1.479416, mae: 5.262400, mean_q: 9.815052, mean_eps: 0.869545\n",
            " 1476/10000: episode: 63, duration: 0.094s, episode steps:  14, steps per second: 148, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 0.776441, mae: 5.253902, mean_q: 10.020721, mean_eps: 0.867835\n",
            " 1490/10000: episode: 64, duration: 0.087s, episode steps:  14, steps per second: 161, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.357 [0.000, 1.000],  loss: 1.775343, mae: 5.390837, mean_q: 10.164216, mean_eps: 0.866575\n",
            " 1501/10000: episode: 65, duration: 0.073s, episode steps:  11, steps per second: 150, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 1.165050, mae: 5.380388, mean_q: 10.151798, mean_eps: 0.865450\n",
            " 1537/10000: episode: 66, duration: 0.229s, episode steps:  36, steps per second: 157, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 1.380089, mae: 5.433341, mean_q: 10.279342, mean_eps: 0.863335\n",
            " 1557/10000: episode: 67, duration: 0.117s, episode steps:  20, steps per second: 171, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 1.165049, mae: 5.514892, mean_q: 10.497400, mean_eps: 0.860815\n",
            " 1591/10000: episode: 68, duration: 0.200s, episode steps:  34, steps per second: 170, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.416128, mae: 5.610289, mean_q: 10.647496, mean_eps: 0.858385\n",
            " 1623/10000: episode: 69, duration: 0.180s, episode steps:  32, steps per second: 178, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.153179, mae: 5.612265, mean_q: 10.703939, mean_eps: 0.855415\n",
            " 1637/10000: episode: 70, duration: 0.085s, episode steps:  14, steps per second: 165, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 1.362851, mae: 5.787793, mean_q: 11.097825, mean_eps: 0.853345\n",
            " 1678/10000: episode: 71, duration: 0.253s, episode steps:  41, steps per second: 162, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.610 [0.000, 1.000],  loss: 1.979962, mae: 5.838096, mean_q: 10.956216, mean_eps: 0.850870\n",
            " 1702/10000: episode: 72, duration: 0.147s, episode steps:  24, steps per second: 163, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.833504, mae: 5.893065, mean_q: 11.079785, mean_eps: 0.847945\n",
            " 1721/10000: episode: 73, duration: 0.121s, episode steps:  19, steps per second: 157, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 1.812694, mae: 5.972533, mean_q: 11.295217, mean_eps: 0.846010\n",
            " 1733/10000: episode: 74, duration: 0.072s, episode steps:  12, steps per second: 166, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 1.124815, mae: 5.922728, mean_q: 11.280750, mean_eps: 0.844615\n",
            " 1752/10000: episode: 75, duration: 0.111s, episode steps:  19, steps per second: 171, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 1.727363, mae: 6.013170, mean_q: 11.416616, mean_eps: 0.843220\n",
            " 1768/10000: episode: 76, duration: 0.104s, episode steps:  16, steps per second: 153, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.688 [0.000, 1.000],  loss: 2.940964, mae: 6.173155, mean_q: 11.475734, mean_eps: 0.841645\n",
            " 1790/10000: episode: 77, duration: 0.130s, episode steps:  22, steps per second: 170, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 1.599396, mae: 6.086119, mean_q: 11.480792, mean_eps: 0.839935\n",
            " 1803/10000: episode: 78, duration: 0.087s, episode steps:  13, steps per second: 149, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 2.414706, mae: 6.206065, mean_q: 11.698687, mean_eps: 0.838360\n",
            " 1898/10000: episode: 79, duration: 0.579s, episode steps:  95, steps per second: 164, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.174936, mae: 6.288468, mean_q: 11.838362, mean_eps: 0.833500\n",
            " 1937/10000: episode: 80, duration: 0.229s, episode steps:  39, steps per second: 170, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.405423, mae: 6.376460, mean_q: 12.190661, mean_eps: 0.827470\n",
            " 1980/10000: episode: 81, duration: 0.259s, episode steps:  43, steps per second: 166, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.184361, mae: 6.553197, mean_q: 12.464749, mean_eps: 0.823780\n",
            " 2005/10000: episode: 82, duration: 0.151s, episode steps:  25, steps per second: 166, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.129357, mae: 6.602935, mean_q: 12.533363, mean_eps: 0.820720\n",
            " 2025/10000: episode: 83, duration: 0.127s, episode steps:  20, steps per second: 158, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 1.797930, mae: 6.617139, mean_q: 12.671985, mean_eps: 0.818695\n",
            " 2049/10000: episode: 84, duration: 0.155s, episode steps:  24, steps per second: 154, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 2.450580, mae: 6.753479, mean_q: 12.797349, mean_eps: 0.816715\n",
            " 2105/10000: episode: 85, duration: 0.326s, episode steps:  56, steps per second: 172, episode reward: 56.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 2.247798, mae: 6.850636, mean_q: 13.051493, mean_eps: 0.813115\n",
            " 2131/10000: episode: 86, duration: 0.156s, episode steps:  26, steps per second: 166, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.623483, mae: 6.904503, mean_q: 13.259318, mean_eps: 0.809425\n",
            " 2169/10000: episode: 87, duration: 0.261s, episode steps:  38, steps per second: 146, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.773863, mae: 7.046387, mean_q: 13.598694, mean_eps: 0.806545\n",
            " 2196/10000: episode: 88, duration: 0.159s, episode steps:  27, steps per second: 170, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.056105, mae: 7.102760, mean_q: 13.632262, mean_eps: 0.803620\n",
            " 2209/10000: episode: 89, duration: 0.086s, episode steps:  13, steps per second: 152, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 2.698979, mae: 7.217957, mean_q: 13.736328, mean_eps: 0.801820\n",
            " 2224/10000: episode: 90, duration: 0.089s, episode steps:  15, steps per second: 169, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 2.881786, mae: 7.298614, mean_q: 13.841111, mean_eps: 0.800560\n",
            " 2246/10000: episode: 91, duration: 0.134s, episode steps:  22, steps per second: 164, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 2.470371, mae: 7.238645, mean_q: 13.752748, mean_eps: 0.798895\n",
            " 2297/10000: episode: 92, duration: 0.296s, episode steps:  51, steps per second: 172, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.503386, mae: 7.375675, mean_q: 14.064325, mean_eps: 0.795610\n",
            " 2306/10000: episode: 93, duration: 0.058s, episode steps:   9, steps per second: 156, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 1.649559, mae: 7.322823, mean_q: 13.988831, mean_eps: 0.792910\n",
            " 2328/10000: episode: 94, duration: 0.133s, episode steps:  22, steps per second: 165, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.409 [0.000, 1.000],  loss: 2.823810, mae: 7.436142, mean_q: 14.116740, mean_eps: 0.791515\n",
            " 2349/10000: episode: 95, duration: 0.130s, episode steps:  21, steps per second: 162, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.966835, mae: 7.512980, mean_q: 14.216449, mean_eps: 0.789580\n",
            " 2421/10000: episode: 96, duration: 0.441s, episode steps:  72, steps per second: 163, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.135641, mae: 7.555533, mean_q: 14.512711, mean_eps: 0.785395\n",
            " 2471/10000: episode: 97, duration: 0.305s, episode steps:  50, steps per second: 164, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.437863, mae: 7.747527, mean_q: 14.840907, mean_eps: 0.779905\n",
            " 2488/10000: episode: 98, duration: 0.103s, episode steps:  17, steps per second: 165, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.235 [0.000, 1.000],  loss: 2.757678, mae: 7.829293, mean_q: 15.005014, mean_eps: 0.776890\n",
            " 2516/10000: episode: 99, duration: 0.177s, episode steps:  28, steps per second: 158, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 3.704188, mae: 7.890523, mean_q: 14.953761, mean_eps: 0.774865\n",
            " 2536/10000: episode: 100, duration: 0.125s, episode steps:  20, steps per second: 160, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 2.583457, mae: 7.936699, mean_q: 15.198407, mean_eps: 0.772705\n",
            " 2575/10000: episode: 101, duration: 0.245s, episode steps:  39, steps per second: 159, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.431873, mae: 8.008903, mean_q: 15.393600, mean_eps: 0.770050\n",
            " 2590/10000: episode: 102, duration: 0.087s, episode steps:  15, steps per second: 172, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 3.165779, mae: 8.115772, mean_q: 15.538470, mean_eps: 0.767620\n",
            " 2612/10000: episode: 103, duration: 0.132s, episode steps:  22, steps per second: 167, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 2.474535, mae: 8.013430, mean_q: 15.359563, mean_eps: 0.765955\n",
            " 2625/10000: episode: 104, duration: 0.086s, episode steps:  13, steps per second: 151, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 4.247543, mae: 8.242362, mean_q: 15.564782, mean_eps: 0.764380\n",
            " 2673/10000: episode: 105, duration: 0.405s, episode steps:  48, steps per second: 119, episode reward: 48.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.363292, mae: 8.216913, mean_q: 15.636779, mean_eps: 0.761635\n",
            " 2709/10000: episode: 106, duration: 0.303s, episode steps:  36, steps per second: 119, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.319002, mae: 8.238803, mean_q: 15.707593, mean_eps: 0.757855\n",
            " 2777/10000: episode: 107, duration: 0.583s, episode steps:  68, steps per second: 117, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.734270, mae: 8.387286, mean_q: 15.951245, mean_eps: 0.753175\n",
            " 2805/10000: episode: 108, duration: 0.256s, episode steps:  28, steps per second: 109, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 2.545654, mae: 8.362146, mean_q: 16.105978, mean_eps: 0.748855\n",
            " 2864/10000: episode: 109, duration: 0.486s, episode steps:  59, steps per second: 121, episode reward: 59.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.532649, mae: 8.555299, mean_q: 16.401410, mean_eps: 0.744940\n",
            " 2909/10000: episode: 110, duration: 0.408s, episode steps:  45, steps per second: 110, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.336972, mae: 8.726206, mean_q: 16.777663, mean_eps: 0.740260\n",
            " 2981/10000: episode: 111, duration: 0.504s, episode steps:  72, steps per second: 143, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.941670, mae: 8.790243, mean_q: 16.963747, mean_eps: 0.734995\n",
            " 3020/10000: episode: 112, duration: 0.245s, episode steps:  39, steps per second: 159, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.572597, mae: 8.874403, mean_q: 17.046884, mean_eps: 0.730000\n",
            " 3031/10000: episode: 113, duration: 0.068s, episode steps:  11, steps per second: 161, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 2.737501, mae: 8.945267, mean_q: 17.354124, mean_eps: 0.727750\n",
            " 3084/10000: episode: 114, duration: 0.323s, episode steps:  53, steps per second: 164, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.228264, mae: 9.046193, mean_q: 17.500072, mean_eps: 0.724870\n",
            " 3113/10000: episode: 115, duration: 0.180s, episode steps:  29, steps per second: 161, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.452229, mae: 9.082719, mean_q: 17.506563, mean_eps: 0.721180\n",
            " 3133/10000: episode: 116, duration: 0.128s, episode steps:  20, steps per second: 156, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.989481, mae: 9.183988, mean_q: 17.761638, mean_eps: 0.718975\n",
            " 3193/10000: episode: 117, duration: 0.364s, episode steps:  60, steps per second: 165, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.721337, mae: 9.277083, mean_q: 17.890975, mean_eps: 0.715375\n",
            " 3239/10000: episode: 118, duration: 0.278s, episode steps:  46, steps per second: 165, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.130387, mae: 9.380781, mean_q: 17.984666, mean_eps: 0.710605\n",
            " 3307/10000: episode: 119, duration: 0.410s, episode steps:  68, steps per second: 166, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.322174, mae: 9.579068, mean_q: 18.422413, mean_eps: 0.705475\n",
            " 3331/10000: episode: 120, duration: 0.152s, episode steps:  24, steps per second: 158, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.456757, mae: 9.614702, mean_q: 18.583839, mean_eps: 0.701335\n",
            " 3360/10000: episode: 121, duration: 0.187s, episode steps:  29, steps per second: 155, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 2.574753, mae: 9.655881, mean_q: 18.882140, mean_eps: 0.698950\n",
            " 3397/10000: episode: 122, duration: 0.228s, episode steps:  37, steps per second: 162, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 3.111065, mae: 9.738690, mean_q: 18.893767, mean_eps: 0.695980\n",
            " 3409/10000: episode: 123, duration: 0.078s, episode steps:  12, steps per second: 153, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 4.088977, mae: 9.772258, mean_q: 18.936281, mean_eps: 0.693775\n",
            " 3459/10000: episode: 124, duration: 0.301s, episode steps:  50, steps per second: 166, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 3.673768, mae: 9.955047, mean_q: 19.288692, mean_eps: 0.690985\n",
            " 3493/10000: episode: 125, duration: 0.199s, episode steps:  34, steps per second: 171, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.780600, mae: 9.980206, mean_q: 19.144928, mean_eps: 0.687205\n",
            " 3519/10000: episode: 126, duration: 0.159s, episode steps:  26, steps per second: 164, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.930212, mae: 10.032334, mean_q: 19.249193, mean_eps: 0.684505\n",
            " 3579/10000: episode: 127, duration: 0.362s, episode steps:  60, steps per second: 166, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.608800, mae: 10.046874, mean_q: 19.482889, mean_eps: 0.680635\n",
            " 3614/10000: episode: 128, duration: 0.228s, episode steps:  35, steps per second: 154, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 5.219957, mae: 10.299876, mean_q: 19.805985, mean_eps: 0.676360\n",
            " 3640/10000: episode: 129, duration: 0.159s, episode steps:  26, steps per second: 163, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.812258, mae: 10.296280, mean_q: 19.771673, mean_eps: 0.673615\n",
            " 3651/10000: episode: 130, duration: 0.074s, episode steps:  11, steps per second: 149, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 3.353589, mae: 10.307042, mean_q: 19.901576, mean_eps: 0.671950\n",
            " 3690/10000: episode: 131, duration: 0.246s, episode steps:  39, steps per second: 159, episode reward: 39.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.564 [0.000, 1.000],  loss: 4.290239, mae: 10.403190, mean_q: 20.131164, mean_eps: 0.669700\n",
            " 3712/10000: episode: 132, duration: 0.137s, episode steps:  22, steps per second: 161, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 5.138721, mae: 10.470283, mean_q: 20.230577, mean_eps: 0.666955\n",
            " 3742/10000: episode: 133, duration: 0.182s, episode steps:  30, steps per second: 165, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.427653, mae: 10.433668, mean_q: 20.216879, mean_eps: 0.664615\n",
            " 3791/10000: episode: 134, duration: 0.300s, episode steps:  49, steps per second: 163, episode reward: 49.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.474393, mae: 10.644155, mean_q: 20.598641, mean_eps: 0.661060\n",
            " 3834/10000: episode: 135, duration: 0.256s, episode steps:  43, steps per second: 168, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.674449, mae: 10.741284, mean_q: 20.935074, mean_eps: 0.656920\n",
            " 3871/10000: episode: 136, duration: 0.232s, episode steps:  37, steps per second: 159, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.568 [0.000, 1.000],  loss: 5.699571, mae: 10.839093, mean_q: 20.852219, mean_eps: 0.653320\n",
            " 4022/10000: episode: 137, duration: 0.940s, episode steps: 151, steps per second: 161, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 4.859889, mae: 10.970555, mean_q: 21.179240, mean_eps: 0.644860\n",
            " 4113/10000: episode: 138, duration: 0.541s, episode steps:  91, steps per second: 168, episode reward: 91.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 3.474798, mae: 11.171773, mean_q: 21.809191, mean_eps: 0.633970\n",
            " 4149/10000: episode: 139, duration: 0.215s, episode steps:  36, steps per second: 168, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 4.870060, mae: 11.416218, mean_q: 22.099382, mean_eps: 0.628255\n",
            " 4214/10000: episode: 140, duration: 0.387s, episode steps:  65, steps per second: 168, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 5.190056, mae: 11.424323, mean_q: 22.154548, mean_eps: 0.623710\n",
            " 4226/10000: episode: 141, duration: 0.073s, episode steps:  12, steps per second: 165, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 3.702388, mae: 11.304149, mean_q: 22.036133, mean_eps: 0.620245\n",
            " 4336/10000: episode: 142, duration: 0.679s, episode steps: 110, steps per second: 162, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.763969, mae: 11.657279, mean_q: 22.659860, mean_eps: 0.614755\n",
            " 4415/10000: episode: 143, duration: 0.476s, episode steps:  79, steps per second: 166, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 4.622713, mae: 11.844366, mean_q: 23.024888, mean_eps: 0.606250\n",
            " 4485/10000: episode: 144, duration: 0.414s, episode steps:  70, steps per second: 169, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 4.879871, mae: 11.946080, mean_q: 23.168562, mean_eps: 0.599545\n",
            " 4634/10000: episode: 145, duration: 1.064s, episode steps: 149, steps per second: 140, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 4.647503, mae: 12.200331, mean_q: 23.801229, mean_eps: 0.589690\n",
            " 4699/10000: episode: 146, duration: 0.539s, episode steps:  65, steps per second: 120, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 5.570208, mae: 12.415961, mean_q: 24.138580, mean_eps: 0.580060\n",
            " 4735/10000: episode: 147, duration: 0.333s, episode steps:  36, steps per second: 108, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.730083, mae: 12.495641, mean_q: 24.524874, mean_eps: 0.575515\n",
            " 4771/10000: episode: 148, duration: 0.315s, episode steps:  36, steps per second: 114, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.412713, mae: 12.645581, mean_q: 24.508640, mean_eps: 0.572275\n",
            " 4826/10000: episode: 149, duration: 0.501s, episode steps:  55, steps per second: 110, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 4.476341, mae: 12.646112, mean_q: 24.632121, mean_eps: 0.568180\n",
            " 4884/10000: episode: 150, duration: 0.459s, episode steps:  58, steps per second: 126, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 5.916587, mae: 12.787223, mean_q: 24.907750, mean_eps: 0.563095\n",
            " 4941/10000: episode: 151, duration: 0.365s, episode steps:  57, steps per second: 156, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 4.762460, mae: 12.850221, mean_q: 25.111621, mean_eps: 0.557920\n",
            " 5031/10000: episode: 152, duration: 0.570s, episode steps:  90, steps per second: 158, episode reward: 90.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 7.072131, mae: 13.060974, mean_q: 25.366221, mean_eps: 0.551305\n",
            " 5101/10000: episode: 153, duration: 0.415s, episode steps:  70, steps per second: 169, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 6.529885, mae: 13.171739, mean_q: 25.604936, mean_eps: 0.544105\n",
            " 5158/10000: episode: 154, duration: 0.355s, episode steps:  57, steps per second: 160, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 6.225866, mae: 13.276157, mean_q: 25.847689, mean_eps: 0.538390\n",
            " 5179/10000: episode: 155, duration: 0.133s, episode steps:  21, steps per second: 158, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 5.845063, mae: 13.325731, mean_q: 25.930632, mean_eps: 0.534880\n",
            " 5258/10000: episode: 156, duration: 0.496s, episode steps:  79, steps per second: 159, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 7.090776, mae: 13.456969, mean_q: 26.129496, mean_eps: 0.530380\n",
            " 5295/10000: episode: 157, duration: 0.248s, episode steps:  37, steps per second: 149, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.459 [0.000, 1.000],  loss: 8.031760, mae: 13.544385, mean_q: 26.243998, mean_eps: 0.525160\n",
            " 5368/10000: episode: 158, duration: 0.456s, episode steps:  73, steps per second: 160, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.842432, mae: 13.513923, mean_q: 26.289546, mean_eps: 0.520210\n",
            " 5451/10000: episode: 159, duration: 0.490s, episode steps:  83, steps per second: 169, episode reward: 83.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 5.967897, mae: 13.676975, mean_q: 26.688186, mean_eps: 0.513190\n",
            " 5531/10000: episode: 160, duration: 0.495s, episode steps:  80, steps per second: 162, episode reward: 80.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 6.825516, mae: 13.861036, mean_q: 26.992941, mean_eps: 0.505855\n",
            " 5601/10000: episode: 161, duration: 0.413s, episode steps:  70, steps per second: 169, episode reward: 70.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 6.444398, mae: 13.954833, mean_q: 27.144134, mean_eps: 0.499105\n",
            " 5630/10000: episode: 162, duration: 0.188s, episode steps:  29, steps per second: 154, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 6.116240, mae: 14.067303, mean_q: 27.423705, mean_eps: 0.494650\n",
            " 5714/10000: episode: 163, duration: 0.500s, episode steps:  84, steps per second: 168, episode reward: 84.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 7.156055, mae: 14.101261, mean_q: 27.397665, mean_eps: 0.489565\n",
            " 5810/10000: episode: 164, duration: 0.567s, episode steps:  96, steps per second: 169, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.681931, mae: 14.249481, mean_q: 27.770358, mean_eps: 0.481465\n",
            " 5867/10000: episode: 165, duration: 0.345s, episode steps:  57, steps per second: 165, episode reward: 57.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 6.484976, mae: 14.296171, mean_q: 27.856630, mean_eps: 0.474580\n",
            " 5963/10000: episode: 166, duration: 0.545s, episode steps:  96, steps per second: 176, episode reward: 96.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 5.841804, mae: 14.423344, mean_q: 28.194631, mean_eps: 0.467695\n",
            " 6058/10000: episode: 167, duration: 0.588s, episode steps:  95, steps per second: 161, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 6.453326, mae: 14.670511, mean_q: 28.663005, mean_eps: 0.459100\n",
            " 6139/10000: episode: 168, duration: 0.477s, episode steps:  81, steps per second: 170, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 6.292980, mae: 14.746899, mean_q: 28.849617, mean_eps: 0.451180\n",
            " 6247/10000: episode: 169, duration: 0.629s, episode steps: 108, steps per second: 172, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 6.450449, mae: 14.914466, mean_q: 29.218007, mean_eps: 0.442675\n",
            " 6260/10000: episode: 170, duration: 0.085s, episode steps:  13, steps per second: 152, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 5.853493, mae: 15.132218, mean_q: 29.700059, mean_eps: 0.437230\n",
            " 6332/10000: episode: 171, duration: 0.436s, episode steps:  72, steps per second: 165, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 6.336413, mae: 15.150629, mean_q: 29.642555, mean_eps: 0.433405\n",
            " 6455/10000: episode: 172, duration: 0.720s, episode steps: 123, steps per second: 171, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 6.975255, mae: 15.353114, mean_q: 30.087196, mean_eps: 0.424630\n",
            " 6569/10000: episode: 173, duration: 0.863s, episode steps: 114, steps per second: 132, episode reward: 114.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 6.711475, mae: 15.508742, mean_q: 30.387290, mean_eps: 0.413965\n",
            " 6675/10000: episode: 174, duration: 0.916s, episode steps: 106, steps per second: 116, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.684392, mae: 15.707757, mean_q: 30.726800, mean_eps: 0.404065\n",
            " 6875/10000: episode: 175, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 6.247542, mae: 15.966663, mean_q: 31.391078, mean_eps: 0.390295\n",
            " 6993/10000: episode: 176, duration: 0.690s, episode steps: 118, steps per second: 171, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 7.431172, mae: 16.222509, mean_q: 31.836895, mean_eps: 0.375985\n",
            " 7121/10000: episode: 177, duration: 0.767s, episode steps: 128, steps per second: 167, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 6.596147, mae: 16.391376, mean_q: 32.298982, mean_eps: 0.364915\n",
            " 7232/10000: episode: 178, duration: 1.051s, episode steps: 111, steps per second: 106, episode reward: 111.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.441 [0.000, 1.000],  loss: 6.961975, mae: 16.666223, mean_q: 32.791970, mean_eps: 0.354160\n",
            " 7335/10000: episode: 179, duration: 0.962s, episode steps: 103, steps per second: 107, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 8.087414, mae: 16.886282, mean_q: 33.185206, mean_eps: 0.344530\n",
            " 7452/10000: episode: 180, duration: 0.708s, episode steps: 117, steps per second: 165, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 6.847504, mae: 17.061813, mean_q: 33.690986, mean_eps: 0.334630\n",
            " 7577/10000: episode: 181, duration: 0.754s, episode steps: 125, steps per second: 166, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 8.700056, mae: 17.228408, mean_q: 33.893192, mean_eps: 0.323740\n",
            " 7718/10000: episode: 182, duration: 0.835s, episode steps: 141, steps per second: 169, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.454 [0.000, 1.000],  loss: 7.890714, mae: 17.471260, mean_q: 34.381047, mean_eps: 0.311770\n",
            " 7795/10000: episode: 183, duration: 0.464s, episode steps:  77, steps per second: 166, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 8.053294, mae: 17.698730, mean_q: 34.939762, mean_eps: 0.301960\n",
            " 7995/10000: episode: 184, duration: 1.185s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 7.163563, mae: 17.878686, mean_q: 35.331091, mean_eps: 0.289495\n",
            " 8016/10000: episode: 185, duration: 0.128s, episode steps:  21, steps per second: 164, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.885224, mae: 18.126237, mean_q: 35.863606, mean_eps: 0.279550\n",
            " 8170/10000: episode: 186, duration: 0.934s, episode steps: 154, steps per second: 165, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 8.709787, mae: 18.235342, mean_q: 35.995129, mean_eps: 0.271675\n",
            " 8370/10000: episode: 187, duration: 1.274s, episode steps: 200, steps per second: 157, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 8.579947, mae: 18.561464, mean_q: 36.615354, mean_eps: 0.255745\n",
            " 8570/10000: episode: 188, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 9.171592, mae: 18.929257, mean_q: 37.304483, mean_eps: 0.237745\n",
            " 8770/10000: episode: 189, duration: 1.383s, episode steps: 200, steps per second: 145, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 9.566696, mae: 19.192856, mean_q: 37.818085, mean_eps: 0.219745\n",
            " 8970/10000: episode: 190, duration: 1.189s, episode steps: 200, steps per second: 168, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.920072, mae: 19.434198, mean_q: 38.506005, mean_eps: 0.201745\n",
            " 9170/10000: episode: 191, duration: 1.194s, episode steps: 200, steps per second: 167, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 9.470044, mae: 19.860480, mean_q: 39.313411, mean_eps: 0.183745\n",
            " 9354/10000: episode: 192, duration: 1.142s, episode steps: 184, steps per second: 161, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 9.605693, mae: 20.205694, mean_q: 39.953706, mean_eps: 0.166465\n",
            " 9554/10000: episode: 193, duration: 1.199s, episode steps: 200, steps per second: 167, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 8.739190, mae: 20.452061, mean_q: 40.520398, mean_eps: 0.149185\n",
            " 9754/10000: episode: 194, duration: 1.182s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 10.374064, mae: 20.754952, mean_q: 41.086191, mean_eps: 0.131185\n",
            " 9954/10000: episode: 195, duration: 1.182s, episode steps: 200, steps per second: 169, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 8.818616, mae: 21.094314, mean_q: 41.902767, mean_eps: 0.113185\n",
            "done, took 66.095 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd5215a3700>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SD0xr5LxZj_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}