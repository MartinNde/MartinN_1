{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinNde/MartinN_1/blob/main/7225_PredictionChallenge3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKK5DA390wRe"
      },
      "source": [
        "# Prediction Challenge 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8ZiiRbxlH2D"
      },
      "source": [],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install keras rl2 (we need to install keras-rl2 so it works with the tensorflow 2 version that comes pre-installed with colab)\n",
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deGfOT2xuFgD",
        "outputId": "6a96ee4a-7471-4d77-f548-f7af6d8e20a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (63.4.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (15.0.6.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (3.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (23.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (2.11.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (1.51.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (4.5.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.40.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.16.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow->keras-rl2) (3.2.2)\n",
            "Installing collected packages: keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrUzv72bwiYP",
        "outputId": "2d61b1f3-aee6-467c-9be8-8f51fc7c0f3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the gym module\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "# import the usual Keras modules for creating deep neural networks\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "ENV_NAME = 'CartPole-v0'\n",
        "env = gym.make(ENV_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WMz2eOtwo3v",
        "outputId": "4ff947f3-65a4-43f3-e055-4c87ab1ae91b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rl\n",
        "from rl.memory import SequentialMemory  # import the exerience replay buffer module\n",
        "from rl.policy import BoltzmannQPolicy  # import the policy\n",
        "from rl.agents.dqn import DQNAgent      # import the DQN agent\n",
        "\n",
        "# setup experience replay buffer\n",
        "# here the sequential memory limit is set up the same as the nb_steps (number of steps)\n",
        "# parameter in the fit method.  This means that all the action-states will fit into the\n",
        "# memory buffer\n",
        "# keep window_length as 1. It's used in other RL methods, but keep it to 1 in DQNs\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# define the policy (how we select the actions)\n",
        "policy = BoltzmannQPolicy()"
      ],
      "metadata": {
        "id": "aFPJ_maZwsf_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of DQN for CartPole, applying policy BoltzmannQPolicy"
      ],
      "metadata": {
        "id": "uNNdoXo4w_CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=10000, window_length=1)\n",
        "\n",
        "# setup the Linear annealed policy with the BoltzmannQPolicy as the inner policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=EpsGreedyQPolicy(),   # policy used to select actions\n",
        "                               attr='eps',                        # attribute in the inner policy to vary             \n",
        "                               value_max=1,                       # maximum value of attribute that is varying\n",
        "                               value_min=0.1,                      # minimum value of attribute that is varying\n",
        "                               value_test=0.05,                    # test if the value selected is < 0.05\n",
        "                               nb_steps=1000)                    # the number of steps between value_max and value_min\n",
        "\n",
        "# Q-Network\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))  # The input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Flatten())\n",
        "model.add(Dense(16, activation='relu'))\n",
        "# add extra layers here\n",
        "model.add(Dense(env.action_space.n, activation='linear'))   # the output is the number of actions in the action space\n",
        "print(model.summary())\n",
        "\n",
        "# define the agent\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\n",
        "               nb_actions=env.action_space.n,   # number of actions\n",
        "               memory=memory,                   # experience replay memory\n",
        "               nb_steps_warmup=10,              # how many steps are waited before starting experience replay\n",
        "               target_model_update=1e-2,        # how often the target network is updated\n",
        "               policy=policy)                   # the action selection policy\n",
        "\n",
        "dqn.compile(Adam(lr=1e-2), metrics=['mae'])\n",
        "\n",
        "history = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)\n",
        "\n",
        "# summarize the history for number  of episode steps\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n",
        "\n",
        "dqn.test(env, nb_episodes=20, visualize=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F1yI7Cuxwxov",
        "outputId": "d0679ad3-9ddd-4636-a75e-77b16628b88c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_5 (Flatten)         (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                80        \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 114\n",
            "Trainable params: 114\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 10 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 11 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 12 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 13 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 14 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 15 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 16 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 17 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 18 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 19 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 20 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 21 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 22 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 23 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 24 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 25 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 26 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 27 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 28 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 29 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 30 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n",
            "/usr/local/lib/python3.9/dist-packages/rl/memory.py:38: DeprecationWarning: This function is deprecated. Please call randint(1, 31 + 1) instead\n",
            "  batch_idxs = np.random.random_integers(low, high - 1, size=size)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   21/10000: episode: 1, duration: 1.778s, episode steps:  21, steps per second:  12, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.336052, mae: 0.555927, mean_q: 0.377402, mean_eps: 0.986050\n",
            "   55/10000: episode: 2, duration: 0.273s, episode steps:  34, steps per second: 124, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.588 [0.000, 1.000],  loss: 0.035498, mae: 0.727316, mean_q: 1.362125, mean_eps: 0.966250\n",
            "   66/10000: episode: 3, duration: 0.085s, episode steps:  11, steps per second: 129, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.027640, mae: 0.864297, mean_q: 1.742531, mean_eps: 0.946000\n",
            "  110/10000: episode: 4, duration: 0.329s, episode steps:  44, steps per second: 134, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.023187, mae: 0.955129, mean_q: 1.930899, mean_eps: 0.921250\n",
            "  157/10000: episode: 5, duration: 0.345s, episode steps:  47, steps per second: 136, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.574 [0.000, 1.000],  loss: 0.031590, mae: 1.131586, mean_q: 2.284309, mean_eps: 0.880300\n",
            "  167/10000: episode: 6, duration: 0.080s, episode steps:  10, steps per second: 125, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.065341, mae: 1.270609, mean_q: 2.477850, mean_eps: 0.854650\n",
            "  181/10000: episode: 7, duration: 0.121s, episode steps:  14, steps per second: 116, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.072876, mae: 1.343032, mean_q: 2.638111, mean_eps: 0.843850\n",
            "  202/10000: episode: 8, duration: 0.150s, episode steps:  21, steps per second: 140, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.069605, mae: 1.444966, mean_q: 2.886911, mean_eps: 0.828100\n",
            "  217/10000: episode: 9, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.080671, mae: 1.563112, mean_q: 3.121482, mean_eps: 0.811900\n",
            "  233/10000: episode: 10, duration: 0.126s, episode steps:  16, steps per second: 127, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.136577, mae: 1.640326, mean_q: 3.223623, mean_eps: 0.797950\n",
            "  247/10000: episode: 11, duration: 0.117s, episode steps:  14, steps per second: 120, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.111588, mae: 1.730370, mean_q: 3.406306, mean_eps: 0.784450\n",
            "  266/10000: episode: 12, duration: 0.142s, episode steps:  19, steps per second: 133, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.146802, mae: 1.803057, mean_q: 3.533046, mean_eps: 0.769600\n",
            "  286/10000: episode: 13, duration: 0.160s, episode steps:  20, steps per second: 125, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.161695, mae: 1.893091, mean_q: 3.644367, mean_eps: 0.752050\n",
            "  299/10000: episode: 14, duration: 0.100s, episode steps:  13, steps per second: 130, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.223103, mae: 2.006739, mean_q: 3.793009, mean_eps: 0.737200\n",
            "  313/10000: episode: 15, duration: 0.115s, episode steps:  14, steps per second: 122, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.180698, mae: 2.034169, mean_q: 3.981808, mean_eps: 0.725050\n",
            "  329/10000: episode: 16, duration: 0.115s, episode steps:  16, steps per second: 139, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.232238, mae: 2.117877, mean_q: 4.081274, mean_eps: 0.711550\n",
            "  344/10000: episode: 17, duration: 0.112s, episode steps:  15, steps per second: 134, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.255606, mae: 2.178379, mean_q: 4.096166, mean_eps: 0.697600\n",
            "  363/10000: episode: 18, duration: 0.142s, episode steps:  19, steps per second: 134, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 0.206768, mae: 2.258312, mean_q: 4.371068, mean_eps: 0.682300\n",
            "  376/10000: episode: 19, duration: 0.104s, episode steps:  13, steps per second: 125, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.198980, mae: 2.311384, mean_q: 4.469838, mean_eps: 0.667900\n",
            "  401/10000: episode: 20, duration: 0.185s, episode steps:  25, steps per second: 135, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.440 [0.000, 1.000],  loss: 0.241967, mae: 2.405749, mean_q: 4.614604, mean_eps: 0.650800\n",
            "  414/10000: episode: 21, duration: 0.109s, episode steps:  13, steps per second: 119, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.349079, mae: 2.493396, mean_q: 4.666001, mean_eps: 0.633700\n",
            "  428/10000: episode: 22, duration: 0.106s, episode steps:  14, steps per second: 132, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.220602, mae: 2.555684, mean_q: 4.925097, mean_eps: 0.621550\n",
            "  439/10000: episode: 23, duration: 0.091s, episode steps:  11, steps per second: 121, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.422987, mae: 2.629556, mean_q: 4.845952, mean_eps: 0.610300\n",
            "  459/10000: episode: 24, duration: 0.148s, episode steps:  20, steps per second: 135, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.323225, mae: 2.668995, mean_q: 4.988834, mean_eps: 0.596350\n",
            "  473/10000: episode: 25, duration: 0.117s, episode steps:  14, steps per second: 119, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.643 [0.000, 1.000],  loss: 0.292534, mae: 2.751865, mean_q: 5.234094, mean_eps: 0.581050\n",
            "  508/10000: episode: 26, duration: 0.259s, episode steps:  35, steps per second: 135, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.307438, mae: 2.827257, mean_q: 5.365257, mean_eps: 0.559000\n",
            "  519/10000: episode: 27, duration: 0.087s, episode steps:  11, steps per second: 127, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.331018, mae: 2.946226, mean_q: 5.611190, mean_eps: 0.538300\n",
            "  545/10000: episode: 28, duration: 0.220s, episode steps:  26, steps per second: 118, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.336457, mae: 2.999635, mean_q: 5.713925, mean_eps: 0.521650\n",
            "  565/10000: episode: 29, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.240818, mae: 3.091521, mean_q: 5.948337, mean_eps: 0.500950\n",
            "  581/10000: episode: 30, duration: 0.135s, episode steps:  16, steps per second: 119, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.365532, mae: 3.155722, mean_q: 5.985029, mean_eps: 0.484750\n",
            "  612/10000: episode: 31, duration: 0.238s, episode steps:  31, steps per second: 130, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.363272, mae: 3.282263, mean_q: 6.257394, mean_eps: 0.463600\n",
            "  630/10000: episode: 32, duration: 0.137s, episode steps:  18, steps per second: 131, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.556 [0.000, 1.000],  loss: 0.592593, mae: 3.381103, mean_q: 6.381074, mean_eps: 0.441550\n",
            "  663/10000: episode: 33, duration: 0.250s, episode steps:  33, steps per second: 132, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 0.512078, mae: 3.487482, mean_q: 6.616611, mean_eps: 0.418600\n",
            "  687/10000: episode: 34, duration: 0.180s, episode steps:  24, steps per second: 133, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.549552, mae: 3.617130, mean_q: 6.828713, mean_eps: 0.392950\n",
            "  711/10000: episode: 35, duration: 0.189s, episode steps:  24, steps per second: 127, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.540129, mae: 3.687489, mean_q: 6.958980, mean_eps: 0.371350\n",
            "  797/10000: episode: 36, duration: 0.614s, episode steps:  86, steps per second: 140, episode reward: 86.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.450061, mae: 3.893988, mean_q: 7.468831, mean_eps: 0.321850\n",
            "  829/10000: episode: 37, duration: 0.237s, episode steps:  32, steps per second: 135, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.557519, mae: 4.129368, mean_q: 7.929508, mean_eps: 0.268750\n",
            "  863/10000: episode: 38, duration: 0.256s, episode steps:  34, steps per second: 133, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.573434, mae: 4.245603, mean_q: 8.156024, mean_eps: 0.239050\n",
            "  944/10000: episode: 39, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: 81.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.605043, mae: 4.482280, mean_q: 8.677403, mean_eps: 0.187300\n",
            "  987/10000: episode: 40, duration: 0.320s, episode steps:  43, steps per second: 134, episode reward: 43.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.589007, mae: 4.747874, mean_q: 9.218553, mean_eps: 0.131500\n",
            " 1049/10000: episode: 41, duration: 0.456s, episode steps:  62, steps per second: 136, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.650934, mae: 4.929150, mean_q: 9.559455, mean_eps: 0.101321\n",
            " 1116/10000: episode: 42, duration: 0.514s, episode steps:  67, steps per second: 130, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 0.607831, mae: 5.216524, mean_q: 10.236499, mean_eps: 0.100000\n",
            " 1291/10000: episode: 43, duration: 1.518s, episode steps: 175, steps per second: 115, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.563176, mae: 5.719019, mean_q: 11.322936, mean_eps: 0.100000\n",
            " 1483/10000: episode: 44, duration: 2.056s, episode steps: 192, steps per second:  93, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.604288, mae: 6.514721, mean_q: 12.935956, mean_eps: 0.100000\n",
            " 1576/10000: episode: 45, duration: 0.745s, episode steps:  93, steps per second: 125, episode reward: 93.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.637412, mae: 7.060446, mean_q: 14.100234, mean_eps: 0.100000\n",
            " 1704/10000: episode: 46, duration: 0.923s, episode steps: 128, steps per second: 139, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.787441, mae: 7.508671, mean_q: 14.913648, mean_eps: 0.100000\n",
            " 1904/10000: episode: 47, duration: 1.481s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.625701, mae: 8.218898, mean_q: 16.481793, mean_eps: 0.100000\n",
            " 2059/10000: episode: 48, duration: 1.132s, episode steps: 155, steps per second: 137, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.781528, mae: 9.025263, mean_q: 18.079164, mean_eps: 0.100000\n",
            " 2259/10000: episode: 49, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 0.776189, mae: 9.683766, mean_q: 19.389038, mean_eps: 0.100000\n",
            " 2412/10000: episode: 50, duration: 1.074s, episode steps: 153, steps per second: 143, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.887912, mae: 10.290251, mean_q: 20.652263, mean_eps: 0.100000\n",
            " 2612/10000: episode: 51, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.132070, mae: 10.938436, mean_q: 21.912362, mean_eps: 0.100000\n",
            " 2812/10000: episode: 52, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.038912, mae: 11.669451, mean_q: 23.483315, mean_eps: 0.100000\n",
            " 3012/10000: episode: 53, duration: 1.913s, episode steps: 200, steps per second: 105, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.184013, mae: 12.399078, mean_q: 24.976520, mean_eps: 0.100000\n",
            " 3212/10000: episode: 54, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 1.661948, mae: 13.205969, mean_q: 26.599192, mean_eps: 0.100000\n",
            " 3412/10000: episode: 55, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.381621, mae: 13.963256, mean_q: 28.174984, mean_eps: 0.100000\n",
            " 3612/10000: episode: 56, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 1.316146, mae: 14.699266, mean_q: 29.693434, mean_eps: 0.100000\n",
            " 3812/10000: episode: 57, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.403304, mae: 15.498419, mean_q: 31.432089, mean_eps: 0.100000\n",
            " 4012/10000: episode: 58, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.146499, mae: 16.258247, mean_q: 32.792825, mean_eps: 0.100000\n",
            " 4212/10000: episode: 59, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.864173, mae: 16.928497, mean_q: 34.276420, mean_eps: 0.100000\n",
            " 4412/10000: episode: 60, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.824156, mae: 17.574831, mean_q: 35.465195, mean_eps: 0.100000\n",
            " 4612/10000: episode: 61, duration: 1.748s, episode steps: 200, steps per second: 114, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.746284, mae: 18.191077, mean_q: 36.682351, mean_eps: 0.100000\n",
            " 4812/10000: episode: 62, duration: 2.026s, episode steps: 200, steps per second:  99, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.060942, mae: 18.824590, mean_q: 38.052861, mean_eps: 0.100000\n",
            " 5012/10000: episode: 63, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 3.077939, mae: 19.517417, mean_q: 39.333509, mean_eps: 0.100000\n",
            " 5212/10000: episode: 64, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.214353, mae: 20.060091, mean_q: 40.493440, mean_eps: 0.100000\n",
            " 5412/10000: episode: 65, duration: 1.438s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.015026, mae: 20.727097, mean_q: 41.922685, mean_eps: 0.100000\n",
            " 5612/10000: episode: 66, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.695731, mae: 21.486628, mean_q: 43.328954, mean_eps: 0.100000\n",
            " 5812/10000: episode: 67, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.932931, mae: 21.954550, mean_q: 44.471632, mean_eps: 0.100000\n",
            " 6012/10000: episode: 68, duration: 1.476s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.607161, mae: 22.556489, mean_q: 45.577681, mean_eps: 0.100000\n",
            " 6212/10000: episode: 69, duration: 1.641s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.415375, mae: 23.105624, mean_q: 46.796041, mean_eps: 0.100000\n",
            " 6412/10000: episode: 70, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.433643, mae: 23.629417, mean_q: 47.745375, mean_eps: 0.100000\n",
            " 6612/10000: episode: 71, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.526446, mae: 24.217526, mean_q: 48.779533, mean_eps: 0.100000\n",
            " 6812/10000: episode: 72, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.499744, mae: 24.692901, mean_q: 49.946659, mean_eps: 0.100000\n",
            " 7012/10000: episode: 73, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.711803, mae: 25.237828, mean_q: 50.857013, mean_eps: 0.100000\n",
            " 7212/10000: episode: 74, duration: 1.509s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.532520, mae: 25.802401, mean_q: 51.952665, mean_eps: 0.100000\n",
            " 7412/10000: episode: 75, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.224110, mae: 26.276419, mean_q: 52.882570, mean_eps: 0.100000\n",
            " 7612/10000: episode: 76, duration: 1.490s, episode steps: 200, steps per second: 134, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.605722, mae: 26.670435, mean_q: 53.528516, mean_eps: 0.100000\n",
            " 7812/10000: episode: 77, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.771267, mae: 26.917404, mean_q: 54.078912, mean_eps: 0.100000\n",
            " 8012/10000: episode: 78, duration: 2.148s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.364232, mae: 27.044532, mean_q: 54.476812, mean_eps: 0.100000\n",
            " 8212/10000: episode: 79, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.799272, mae: 27.633949, mean_q: 55.510885, mean_eps: 0.100000\n",
            " 8412/10000: episode: 80, duration: 1.453s, episode steps: 200, steps per second: 138, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.009369, mae: 28.030296, mean_q: 56.385310, mean_eps: 0.100000\n",
            " 8612/10000: episode: 81, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.771529, mae: 28.205351, mean_q: 56.908122, mean_eps: 0.100000\n",
            " 8812/10000: episode: 82, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.899960, mae: 28.586218, mean_q: 57.443746, mean_eps: 0.100000\n",
            " 9012/10000: episode: 83, duration: 1.468s, episode steps: 200, steps per second: 136, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 7.809841, mae: 28.997838, mean_q: 58.307738, mean_eps: 0.100000\n",
            " 9212/10000: episode: 84, duration: 1.435s, episode steps: 200, steps per second: 139, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.237884, mae: 29.389001, mean_q: 59.182266, mean_eps: 0.100000\n",
            " 9412/10000: episode: 85, duration: 1.606s, episode steps: 200, steps per second: 125, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.792719, mae: 29.719168, mean_q: 59.763282, mean_eps: 0.100000\n",
            " 9612/10000: episode: 86, duration: 2.145s, episode steps: 200, steps per second:  93, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 6.358663, mae: 30.028955, mean_q: 60.357264, mean_eps: 0.100000\n",
            " 9812/10000: episode: 87, duration: 1.581s, episode steps: 200, steps per second: 127, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.806340, mae: 30.445156, mean_q: 61.263563, mean_eps: 0.100000\n",
            "done, took 80.329 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 200.000, steps: 200\n",
            "Episode 2: reward: 200.000, steps: 200\n",
            "Episode 3: reward: 200.000, steps: 200\n",
            "Episode 4: reward: 200.000, steps: 200\n",
            "Episode 5: reward: 200.000, steps: 200\n",
            "Episode 6: reward: 200.000, steps: 200\n",
            "Episode 7: reward: 200.000, steps: 200\n",
            "Episode 8: reward: 200.000, steps: 200\n",
            "Episode 9: reward: 200.000, steps: 200\n",
            "Episode 10: reward: 200.000, steps: 200\n",
            "Episode 11: reward: 200.000, steps: 200\n",
            "Episode 12: reward: 200.000, steps: 200\n",
            "Episode 13: reward: 200.000, steps: 200\n",
            "Episode 14: reward: 200.000, steps: 200\n",
            "Episode 15: reward: 200.000, steps: 200\n",
            "Episode 16: reward: 200.000, steps: 200\n",
            "Episode 17: reward: 200.000, steps: 200\n",
            "Episode 18: reward: 200.000, steps: 200\n",
            "Episode 19: reward: 200.000, steps: 200\n",
            "Episode 20: reward: 200.000, steps: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f31450fe9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes I made :\n",
        "\n",
        "I changed the learning rate to 1e-2 and nub_step to 1000 - b/c a slower annealing schdedule can help the agent explore more thorougly, leading to a better performance. "
      ],
      "metadata": {
        "id": "as4eO5iZUp6W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZJur1Q5VawQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}